{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2019 Semester 1\n",
    "-----\n",
    "## Project 1: Gaining Information about Naive Bayes\n",
    "-----\n",
    "###### Student Name(s): Akira and Callum\n",
    "###### Python version: 3.7.1 from Anaconda \n",
    "###### Submission deadline: 1pm, Fri 5 Apr 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you may use for your Project 1 submission. (You are not required to use it; in particular, there is no need to use iPython if you do not like it.)\n",
    "\n",
    "Marking will be applied on the five functions that are defined in this notebook, and to your responses to the questions at the end of this notebook.\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The cell below supresses forced output scrolling so you can see view the script output easier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We have specifically imported all functions below to ensure we implemented iteratively AND without the use of external functions wherever possible**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv, DataFrame, Series\n",
    "from collections import defaultdict\n",
    "from numpy import NaN\n",
    "from math import log\n",
    "\n",
    "########## POSSIBLE CSVs ##########\n",
    "d1 =  'anneal.csv'\n",
    "h1 = 'family,product-type,steel,carbon,hardness,temper_rolling,condition,formability,strength,non-ageing,surface-finish,surface-quality,enamelability,bc,bf,bt,bw-me,bl,m,chrom,phos,cbond,marvi,exptl,ferro,corr,bbvc,lustre,jurofm,s,p,shape,oil,bore,packing,class'.split(',')\n",
    "\n",
    "d2 =  'breast-cancer.csv'\n",
    "h2 = 'age,menopause,tumor-size,inv-nodes,node-caps,deg-malig,breast,breast-quad,irradiat,class'.split(',')\n",
    "\n",
    "d3 =  'car.csv'\n",
    "h3 = 'buying,maint,doors,persons,lug_boot,safety,class'.split(',')\n",
    "\n",
    "d4 =  'cmc.csv'\n",
    "h4 = 'w-education,h-education,n-child,w-relation,w-work,h-occupation,standard-of-living,media-exposure,class'.split(',')\n",
    "\n",
    "d5 =  'hepatitis.csv'\n",
    "h5 = 'sex,steroid,antivirals,fatigue,malaise,anorexia,liver-big,liver-firm,spleen-palpable,spiders,ascites,varices,histology,class'.split(',')\n",
    "\n",
    "d6 =  'hypothyroid.csv'\n",
    "h6 = 'sex,on-thyroxine,query-on-thyroxine,on_antithyroid,surgery,query-hypothyroid,query-hyperthyroid,pregnant,sick,tumor,lithium,goitre,TSH,T3,TT4,T4U,FTI,TBG,class'.split(',')\n",
    "\n",
    "d7 =  'mushroom.csv'\n",
    "h7 = 'cap-shape,cap-surface,cap-color,bruises,odor,gill-attachment,gill-spacing,gill-size,gill-color,stalk-shape,stalk-root,stalk-surface-above-ring,stalk-surface-below-ring,stalk-color-above-ring,stalk-color-below-ring,veil-type,veil-color,ring-number,ring-type,spore-print-color,population,habitat,class'.split(',')\n",
    "\n",
    "d8 =  'nursery.csv'\n",
    "h8 = 'parents,has_nurs,form,children,housing,finance,social,health,class'.split(',')\n",
    "\n",
    "d9 = 'primary-tumor.csv'\n",
    "h9 = 'age,sex,histologic-type,degree-of-diffe,bone,bone-marrow,lung,pleura,peritoneum,liver,brain,skin,neck,supraclavicular,axillar,mediastinum,abdominal,class'.split(',')\n",
    "\n",
    "datasets = [d1,d2,d3,d4,d5,d6,d7,d8,d9]\n",
    "dataset_headers = [h1,h2,h3,h4,h5,h6,h7,h8,h9]\n",
    "\n",
    "dictionary = {datasets[i] : dataset_headers[i] for i in range(len(datasets))}\n",
    "\n",
    "\"\"\"Sets the column of each dataset\"\"\"\n",
    "def set_column(filename):\n",
    "    return dictionary[filename]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Has been adjusted so that it works with testing on the train data, and partitioning for cross_val\"\"\"\n",
    "def preprocess(filename, testing_on_train = True, k = 10, drop = 'no', impute = 'no'):\n",
    "    # Add column headers, drop columns with only one unique value (all same value)\n",
    "    df = read_csv(filename, header = None, names = set_column(filename))\n",
    "    df.replace('?', NaN, inplace=True)\n",
    "    \n",
    "    if impute in 'yesYesYES':    \n",
    "        mode = df.mode().iloc[0]\n",
    "        df = df.fillna(mode)\n",
    "    \n",
    "    \"\"\"If we are not using the cross validation method\"\"\"\n",
    "    # Return the whole dataset as a dataframe\n",
    "    if testing_on_train:\n",
    "        return df\n",
    "    else:\n",
    "        \"\"\"Drop no gain attributes given the input in the script\"\"\"\n",
    "        if drop in 'yesYesYES':\n",
    "            non_unique = df.apply(Series.nunique)\n",
    "            df.drop(non_unique[non_unique == 1].index, axis=1, inplace=True)\n",
    "            \n",
    "        temp = df.copy()\n",
    "        partitions = list()\n",
    "        \n",
    "        # k-fold Cross Validation\n",
    "        divisor = k\n",
    "        \n",
    "        for i in range(k):\n",
    "            partitions.append(temp.sample(frac=1/divisor))\n",
    "            divisor -= 1\n",
    "            temp.drop(partitions[-1].index, axis=0, inplace=True)\n",
    "        \n",
    "        del temp\n",
    "        \n",
    "        # Dictionary of train/test pairs\n",
    "        cross_validation_pairs = defaultdict(list)\n",
    "        models = list()\n",
    "        \n",
    "        for i in range(k):\n",
    "            test = partitions[i]\n",
    "            train = df.iloc[df.index.drop(test.index.values)]\n",
    "            cross_validation_pairs[\"train\"].append(train)\n",
    "            cross_validation_pairs[\"test\"].append(test)\n",
    "            \n",
    "        return cross_validation_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Trains a model given a training dataset\"\"\"\n",
    "def train(train_set):\n",
    "    N = len(train_set)\n",
    "    priors = {}\n",
    "    posteriors = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))\n",
    "    \"\"\"Accessable using posteriors[class j][attribute x][value i]\"\"\"\n",
    "    \n",
    "    for label in train_set['class'].unique():\n",
    "        priors[label] = len(train_set.loc[train_set['class'] == label]) / N\n",
    "        for attribute in train_set.columns[:-1]:\n",
    "            temp = train_set.loc[train_set['class'] == label, [attribute,'class']]\n",
    "            n = len(temp)\n",
    "            \n",
    "            count = defaultdict(int)\n",
    "            for i in temp[attribute]:\n",
    "                if i != NaN:\n",
    "                    count[i] += 1\n",
    "            \n",
    "            for i in count:\n",
    "                posteriors[label][attribute][i] = count[i] / n\n",
    "                \n",
    "    trained_model = {\"priors\": priors, \"posteriors\": posteriors}\n",
    "    \n",
    "    return trained_model\n",
    "\n",
    "\"\"\"Trains M partitions for cross_val\"\"\"\n",
    "def cross_validation_train(cross_validation_pairs):\n",
    "    trained_models = list()\n",
    "    train_set = cross_validation_pairs[\"train\"]\n",
    "    test_set = cross_validation_pairs[\"test\"]\n",
    "    N = len(train_set)\n",
    "    \n",
    "    for i in range(N):\n",
    "        trained_models.append(train(train_set[i]))\n",
    "        \n",
    "    return trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Predicts a test set\"\"\"\n",
    "def predict(trained_model, test_set):\n",
    "    priors = trained_model[\"priors\"]\n",
    "    posteriors = trained_model[\"posteriors\"]\n",
    "    \n",
    "    \"\"\"Drop the class labels of the test set\"\"\"\n",
    "    test_labels = test_set['class']\n",
    "    test = test_set.drop('class', axis=1)\n",
    "    cols = test_set.columns\n",
    "    \n",
    "    \"\"\"Probabilistic Smoothing with epsilon -> 0\"\"\"\n",
    "    n = len(test_labels)\n",
    "    epsilon = 1e-100\n",
    "    \n",
    "    \"\"\"Model Prediction\"\"\"\n",
    "    prediction = {}\n",
    "    \n",
    "    \"\"\"The Predicted Labels to be Returned\"\"\"\n",
    "    \"\"\"(Key, Value) = (Test Instance Row, Predicted Label)\"\"\"\n",
    "    predicted_labels = {}\n",
    "    \n",
    "    for i in range(n):\n",
    "        instance = test.iloc[i]\n",
    "        for label in priors.keys():       \n",
    "            prob = log(priors[label])/log(2)\n",
    "            for attribute in cols:\n",
    "                try:\n",
    "                    \"\"\"If the value is non missing\"\"\"\n",
    "                    if instance[attribute] != NaN:\n",
    "                        prob += log(posteriors[label][attribute][instance[attribute]])/log(2)\n",
    "                    else:\n",
    "                        \"\"\"Otherwise we have chosen to simply ignore it\"\"\"\n",
    "                        pass\n",
    "                except:\n",
    "                    \"\"\"If the value does not exist in our model, we use epsilon\"\"\"\n",
    "                    prob += log(epsilon)/log(2)\n",
    "                    \n",
    "            prediction[label] = prob\n",
    "        \n",
    "        \"\"\"Choose the predicted class with the highest probability\"\"\"\n",
    "        predicted_labels[i] = max(prediction, key=prediction.get)\n",
    "        \n",
    "    return predicted_labels\n",
    "\n",
    "\"\"\"Tests partitions\"\"\"\n",
    "def cross_validation_predict(trained_models, cross_validation_pairs):\n",
    "    test_set = cross_validation_pairs[\"test\"]\n",
    "    N = len(test_set)\n",
    "    predictions = list()\n",
    "    \n",
    "    for i in range(N):\n",
    "        predictions.append(predict(trained_models[i], test_set[i]))\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Evaluates the accuracy\"\"\"\n",
    "def evaluate(predicted_labels, test_set):\n",
    "    test_labels = test_set['class']\n",
    "    n = len(test_labels)\n",
    "    \n",
    "    return [1 if predicted_labels[i] == test_labels.iloc[i] else 0 for i in range(n)]\n",
    "\n",
    "\"\"\"Evaluates cross_val accuracy\"\"\"\n",
    "def cross_validation_evaluate(predictions, cross_pairs):\n",
    "    test = cross_pairs[\"test\"]\n",
    "    N = len(test)\n",
    "    results = list()\n",
    "    for i in range(N):\n",
    "        results.append(evaluate(predictions[i], test[i]))\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Calculates the entropy given a series\"\"\"\n",
    "def entropy(distribution):\n",
    "    # Calculates the entropy (uncertainty) of an event (class) for a given distribution \n",
    "    # (e.g. distribution given a certain attribute value)\n",
    "    N = len(distribution)\n",
    "    # Normalise and count values iteratively\n",
    "    count = defaultdict(int)\n",
    "    for i in distribution:\n",
    "        count[i] += 1\n",
    "    event = [i/N for i in count.values()]\n",
    "    \n",
    "    return (-1*sum([i*log(i)/log(2) for i in event]))\n",
    "\n",
    "\"\"\"Calculates the mean information given a dataset\"\"\"\n",
    "def mean_info(dataset):\n",
    "    mean_info_per_attribute = defaultdict(float)\n",
    "    \n",
    "    for attribute in dataset.columns[:-1]:\n",
    "        # Used to calculate 'probability' of said value for a random instance (number of values / total number of instances)\n",
    "        \n",
    "        attribute_value_counts = defaultdict(int)\n",
    "        for i in dataset[attribute]:\n",
    "            attribute_value_counts[i] += 1\n",
    "            \n",
    "        N = len(dataset[attribute])\n",
    "        for value in dataset[attribute].unique():\n",
    "            # dataframe.loc on said value and return the corresponding class column\n",
    "            corresponding_values = dataset.loc[dataset[attribute] == value, 'class']\n",
    "            # add (probability of said value * entropy of said value) to the attribute's mean info\n",
    "            mean_info_per_attribute[attribute] += attribute_value_counts[value]/N * entropy(corresponding_values)\n",
    "            \n",
    "    return mean_info_per_attribute\n",
    "\n",
    "\"\"\"Calculates the information gain given a dataset. Adjusted so that it can drop 0 info_gain columns\"\"\"\n",
    "def info_gain(dataset, drop_no_gain = False):\n",
    "    mean_info_per_attribute = mean_info(dataset) # Weighted sum of child stump entropies if split on each attribute\n",
    "    class_entropy = entropy(dataset['class']) # H(R), the entropy of class distribution prior to splitting\n",
    "    info_gain_given_class = defaultdict(float)\n",
    "    \n",
    "    for attribute in mean_info_per_attribute:\n",
    "        # Calculates IG(attribute) for the dataset's class\n",
    "        info_gain_given_class[attribute] = class_entropy - mean_info_per_attribute[attribute]\n",
    "    \n",
    "    \"\"\"If we want to drop the columns with absolutely 0 information gain\"\"\"\n",
    "    if drop_no_gain:\n",
    "        non_unique = dataset.apply(Series.nunique)\n",
    "        dataset.drop(non_unique[non_unique == 1].index, axis=1, inplace=True)\n",
    "        return dataset\n",
    "    else:\n",
    "        return info_gain_given_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entropy_attributes(distribution):\n",
    "    N = len(distribution)\n",
    "    \n",
    "    count = defaultdict(int)\n",
    "    for i in distribution:\n",
    "        count[i] += 1\n",
    "    event = [i/N for i in count.values()]\n",
    "    \n",
    "    return (-1*sum([i*log(i)/log(2) for i in event]))\n",
    "\n",
    "def mean_info_attributes(dataset, column_name):\n",
    "    mean_info_per_attribute = defaultdict(float)\n",
    "    \n",
    "    # We want every OTHER attribute \n",
    "    cols = [i for i in dataset.columns if i != column_name]\n",
    "    \n",
    "    for attribute in cols: # class label has been dropped already\n",
    "        \n",
    "        attribute_value_counts = defaultdict(int)\n",
    "        for i in dataset[attribute]:\n",
    "            attribute_value_counts[i] += 1\n",
    "            \n",
    "        N = len(dataset[attribute])\n",
    "        for value in dataset[attribute].unique():\n",
    "            corresponding_values = dataset.loc[dataset[attribute] == value, attribute]\n",
    "            mean_info_per_attribute[attribute] += attribute_value_counts[value]/N * entropy_attributes(corresponding_values)\n",
    "            \n",
    "    return mean_info_per_attribute\n",
    "\n",
    "def info_gain_attributes(dataset, to_print = 'no'):\n",
    "    temp = dataset.drop('class', axis=1)\n",
    "    \n",
    "    entropy_per_attribute = defaultdict(float)\n",
    "    all_IG = defaultdict(float) # Me change\n",
    "    \n",
    "    for attribute in temp.columns:\n",
    "        # a dictionary of H(R)\n",
    "        entropy_per_attribute[attribute] = entropy(temp[attribute])\n",
    "        \n",
    "        mean_info_per_attribute = mean_info_attributes(temp, attribute)\n",
    "        \n",
    "    for attribute in entropy_per_attribute:\n",
    "        if to_print in 'yesYesYES': # Me change\n",
    "            print(f\"Entropy({attribute}) = {entropy_per_attribute[attribute]:.4f}\\n\")\n",
    "        average = list()\n",
    "        for every_other_attribute in mean_info_per_attribute:\n",
    "            info = entropy_per_attribute[attribute] - mean_info_per_attribute[every_other_attribute]\n",
    "            average.append(info)\n",
    "            if to_print in 'yesYesYES':\n",
    "                print(f\"InfoGain({every_other_attribute} | {attribute}) = {info:.4f}\")\n",
    "        \n",
    "        if to_print in 'yesYesYES': # Me change\n",
    "            print(f\"\\nAverage Info Gain for {attribute} to every other attribute is {sum(average)/len(mean_info_per_attribute):.4f}\")\n",
    "            print(\"*\"*40)\n",
    "        \n",
    "        all_IG[attribute]= sum(average)/len(mean_info_per_attribute) # Me change\n",
    "    return all_IG # Me change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here is a script that runs every dataset and tests on its training, as well as k-fold cross-validation for a given _k_. It will also ask (y/n) for printing relevant information gain, and to drop attributes with 0 information gain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "Enter k value for k-Fold Cross Validation: 10\n",
      "Drop all columns with absolutely no information gain? (y/n): n\n",
      "Print the information gain for class? (y/n): n\n",
      "Impute missing values? (y/n): n\n",
      "****************************************\n",
      "Processing anneal.csv ...\n",
      "For anneal.csv:\n",
      "Number of rows: 898\n",
      "Number of attributes/columns: 36\n",
      "Information Gain for Class:\n",
      "Average: 0.0882, Best 0.4352, Worst 0.0000, Variance 0.0143\n",
      "Interdependence Between Attribute Pairs:\n",
      "Average: 0.4917, Best 2.0508, Worst 0.0000, Vairance 0.3170\n",
      "\n",
      "TESTING ON THE TRAIN DATA\n",
      "Accuracy for Testing on the Training Data: 99.11%\n",
      "\n",
      "10-FOLD CROSS VALIDATION\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "print(\"*\"*40)\n",
    "k = int(input(\"Enter k value for k-Fold Cross Validation: \"))\n",
    "\"\"\"DO WE WANT TO DROP 0 INFO GAIN ATTRIBUTES?\"\"\"\n",
    "drop = input(\"Drop all columns with absolutely no information gain? (y/n): \").lower()\n",
    "\"\"\"DO WE WANT TO PRINT THE INFO GAIN?\"\"\"\n",
    "to_print = input(\"Print the information gain for class? (y/n): \").lower()\n",
    "\"\"\"DO WE WANT TO IMPUTE MISSING VALUES FOR THE TRAINING SET\"\"\"\n",
    "to_impute = input(\"Impute missing values? (y/n): \").lower()\n",
    "print(\"*\"*40)\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "from numpy import var, array\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ig = defaultdict(float)\n",
    "acc = defaultdict(float)\n",
    "\n",
    "avg_ig = defaultdict(float)\n",
    "\n",
    "for data in datasets:\n",
    "    print(f\"Processing {data} ...\")\n",
    "    \n",
    "    df = preprocess(data, impute = to_impute)\n",
    "    if drop in 'yesYesYES':\n",
    "        df = info_gain(df, drop_no_gain = True)\n",
    "        info_gain_given_class = info_gain(df)\n",
    "    else:\n",
    "        info_gain_given_class = info_gain(df)\n",
    "    if to_print in 'yesYesYES':\n",
    "        for attribute in info_gain_given_class:\n",
    "            print(f'InfoGain({attribute} | class) = {info_gain_given_class[attribute]:.4f}')\n",
    "        print('...')\n",
    "\n",
    "    IG_per_attr = info_gain_attributes(df)\n",
    "    \n",
    "    avg_ig[data] = sum(info_gain_given_class.values())/len(info_gain_given_class)\n",
    "    \n",
    "    print(f\"For {data}:\") # Me change\n",
    "    print(\"Number of rows: %d\" % len(df))\n",
    "    print(\"Number of attributes/columns: %d\" % len(df.columns))\n",
    "    print(\"Information Gain for Class:\")\n",
    "    print(f\"Average: {sum(info_gain_given_class.values())/len(info_gain_given_class):.4f}, Best {max(info_gain_given_class.values()):.4f}, Worst {min(info_gain_given_class.values()):.4f}, Variance {var(array(list(info_gain_given_class.values()))):.4f}\")\n",
    "    print(\"Interdependence Between Attribute Pairs:\")\n",
    "    print(f\"Average: {sum(IG_per_attr.values())/len(IG_per_attr):.4f}, Best {max(IG_per_attr.values()):.4f}, Worst {min(IG_per_attr.values()):.4f}, Vairance {var(array(list(IG_per_attr.values()))):.4f}\") # Me change\n",
    "\n",
    "    ig[data] = sum(IG_per_attr.values())/len(IG_per_attr)\n",
    "    \n",
    "    print(\"\\nTESTING ON THE TRAIN DATA\")\n",
    "    \n",
    "\n",
    "\n",
    "    \"\"\"TRAIN / TEST\"\"\"\n",
    "    model = train(df)\n",
    "    prediction = predict(model, df)\n",
    "    results = evaluate(prediction, df)\n",
    "    print(f\"Accuracy for Testing on the Training Data: {100*sum(results)/len(results):.2f}%\")\n",
    "    \n",
    "    acc[data] = 100*sum(results)/len(results)\n",
    "\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "    print(f\"\\n{k}-FOLD CROSS VALIDATION\")\n",
    "\n",
    "    cross_validation_pairs = preprocess(data, testing_on_train = False, k = k, drop = drop)\n",
    "    trained_models = cross_validation_train(cross_validation_pairs)\n",
    "    predictions = cross_validation_predict(trained_models, cross_validation_pairs)\n",
    "    cross_validation_results = cross_validation_evaluate(predictions, cross_validation_pairs)\n",
    "    print(\"Accuracy using k-Fold Cross Validation: \" + \" \".join([f\"{100*sum(i) / len(i):.2f}%\" for i in cross_validation_results]))\n",
    "    print(f\"Average {k}-Fold Cross Validation Accuracy: {sum([100*sum(i) / len(i) for i in cross_validation_results]) / len(cross_validation_results):.2f}%\")\n",
    "    print(\"*\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 1 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.099463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.099463</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0  1.000000 -0.099463\n",
       "1 -0.099463  1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mapper = defaultdict(list)\n",
    "for i in avg_ig.keys():\n",
    "    mapper[i].append(avg_ig[i])\n",
    "    mapper[i].append(acc[i])\n",
    "mapper.values()\n",
    "\n",
    "plt.scatter([i[0] for i in mapper.values()], [i[1] for i in mapper.values()])\n",
    "plt.title(\"Information Gain\")\n",
    "plt.xlabel(\"Average IG for class\")\n",
    "plt.ylabel(\"Naive Bayes Accuracy\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "display(DataFrame.from_dict(mapper).T.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 2 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xu8FXW9//HXW0DdXlFBE0yhUsy0xKi8nK56Iu0imqakqWlZndLyoaT0M7ueMjmdysyKstRKkxLxVpJRmp28oah4Iz2CygaVVLxuj4Cf3x/f75LFdvZeA+y1ZsF+Px+P/Vgz35k181lr7z2f+X5n5vtVRGBmZtbdOlUHYGZm7ckJwszMCjlBmJlZIScIMzMr5ARhZmaFnCDMzKyQE4SVJulLkn5edRy2+taG36WkrST9TdIzkr5bdTxrIyeIfkTSPEmPStqwruwTkq4p8/6I+FZEfKIJcV0j6QVJz0p6Kv/T79LX+1ldkkZKeknS2VXHsjIkvUvS/PqyJv4uj5K0LP8un5Z0m6QP9PV+smOBfwGbRMSJTdpHv+YE0f8MBD5fdRAFPhcRGwFbANcAv6o2nEJHAE8Ch0par+pg2tj1+Xc5GDgHmCJp8+4rSRq4mvvZDrg7/LRv0zhB9D+TgJMkDS5aKOkHkh7OZ3+3SHp73bKvSvp1nr5K0ue6vfd2SQfm6R0lXS3pCUlzJH2kTHARsRT4LbBT3XbfKul6SYslLZR0lqR187IfdW9ekHS5pC/k6WGSLpa0SNJcScd32+7M/FkflfTfDcI7AjgVWAJ8sG47kvQ9SY/lGtAdknbOy/aTdHduBumUdFIuP0rS37vFHZJel6fPlXS2pD/ms/H/kfQqSd+X9KSkeyWNrnvvPEkT876elPRLSevn2uIfgWF5O8/m7+Tl32V+/4ck3ZW/42skvb7btk/Kn+spSRdJWr/Bd0VEvAT8AugAXlOryUg6WdIjwC/z9j8p6f78t3KZpGF1+95T0s15vzdL2rP2/QBHAl/Mn2mfRvHYynOC6H9mks7QT+ph+c3ArsDmwAXA73o4GFwAjK/NSNqJdEZ3ZT4oXZ3X2TKvd7akNzQKLh/4DwNuqCteBpwADAH2APYG/iMvOw8YL2md/P4hefmFuexy4HZgeC7/gqSx+b0/AH4QEZsArwWm9BLX24FtSMlrCilZ1LwXeAewA+ms+RDg8bzsHOBTEbExsDPwl0bfQZ2PkBLSEOD/gOuBW/P874HuCe0wYGz+LDsAp0bEc8C+wIKI2Cj/LOj22XYALgS+AAwF/gBcXkvCdbG8DxgJvBE4qlHwuYbwCeBZ4L5c/CrS39Z2wLGS3gN8O29/a+BB0ndMrnVcCZxJqln+N+nva4uIOAr4DXBG/kx/bhSPrTwniP7pNOA4SUO7L4iIX0fE4xGxNCK+C6wHjCrYxiXArpK2y/OHAVMj4v+ADwDzIuKXeTu3AhcDB/US05mSFpMOJp8DvlYX0y0RcUPe1jzgp8A787KbgKdIB3+AQ4FrIuJR4C3A0Ij4ekS8GBEPAD/L60CqCbxO0pCIeDYi6pNSd0cCf4yIJ0mJb19JW9ZtZ2NgR0ARcU9ELKxbtpOkTSLiyfxdlHVJ/uwvkL7vFyLi/IhYBlwEjO62/lkR8XBEPAH8J3UJvIFDgCsj4uqIWAL8F+msf8+6dc6MiAV525eTTiJ6snv+XT6SYzggIp7Ky14CvhIR/xcRXaS/m19ExK35b2cisIekEcD7gfsi4lf5d38hcC91tTdrLieIfigi7gSuAE7pvkzSiZLuyVX6xcCmpDPW7tt4hnR2VzvYHko6o4N0dvi23FyxOG/nMNLZY0+Oj4jBwPqkBPN7SW/MMe0g6QpJj0h6GvhWt5jOAw7P04ez/PrFdqSmlfo4vgRslZcfQzrTvjc3XxReTJXUARxc+3wRcT3wEPDRPP8X4CzgR8CjkiZL2iS//cPAfsCDkq6VtEcv30F3j9ZNdxXMb9Rt/Yfrph8EhlHOsLw+8HLT0MOkWlfNI3XTzxfsu94NETE4IoZExO7dzu4X5YTX076fJdW+hndflj3YLS5rIieI/usrwCep+2fLzSgnk6r7m+UD9lOAetjGhaTmnT1IZ5x/zeUPA9fmg0TtZ6OI+EyjoCLipYi4Drif1HQD8GPSmeP2uTnoS91i+jWwv6Q3Aa8HptXFMbdbHBtHxH55X/dFxHhSM9h3SElpQ17pAGATUjPZI7n9fDh1zUwRcWZEvBl4AynpTMjlN0fE/nkf01jejPUcsEHt/ZJ6S55lvbpuelug1pTU6CLuAlIyrcWivK3OPoipu+6xdN/3hqTmpM7uy7JtmxSXFXCC6Kci4n5SM8XxdcUbA0uBRcBASaeRDow9+QPpH/jrwEX5zBNS7WQHSR+TNCj/vKX+wmdvcsLZCbirLq6ngWcl7QiskGgiYj7p2smvgItz0wXATcDT+aJoh6QBknaW9Ja8n8MlDc1xL87vWVYQ0pGki627kJpWdgX2IjWx7ZI/29skDSId+F8AlklaV9JhkjbNTTdP123/duANknbN13i+Wua7aeCzkrbJbfdfIv1+IdU8tpC0aQ/vmwK8X9Le+TOcSLrm8Y8+iKmRC4CP5+9hPVLt8MbclPgH0t/RRyUNlHQI6e/iihbEZThB9HdfB+rPmKeT7nj5J6kq/wIrNlusILcZTwX2If2j18qfIZ39H0o6C3yEdIbe262hZ9XusiEd6E+NiD/mZSeRmnOeIV1DuKjg/eeRDuAv3x6b2+o/SDqgzyXdM/9zUrMZpIuud+V9/gA4tFvzB5JqF7e/HxGP1P3cAlxFSh6b5LieJH1vj5Pa8QE+BszLTWOfJjeFRcQ/Sd//n0kXcFe4o2kVXQD8CXgg/3wz7+teUm3vgdzUtkLTU0TMyXH9kPQdfRD4YES82Acx9SoiZgBfJl2jWki6wH5oXvY4qbnxRNJ3+kXgAxHxr2bHZYl8C7GtDSS9g9TUNKKuJtNvSJoHfMJ381hfcg3C1ni5WeTzwM/7Y3IwaxYnCFuj5esai0n30H+/4nDM1ipuYjIzs0KuQZiZWaHV7SyrUkOGDIkRI0ZUHYaZ2Rrllltu+VdEvKInhe7W6AQxYsQIZs6cWXUYZmZrFEndn1Av5CYmMzMr5ARhZmaFnCDMzKyQE4SZmRVqWoKQ9AulEbburCvbXGmUsfvy62a5XJLOVBpV6g5JuzUrLjMzK6eZNYhzSZ2h1TsFmBER2wMzWD4ewb7A9vnnWFL3zmbWT0yb1clep/+FkadcyV6n/4Vps9yjdztoWoKIiL8BT3Qr3p/U6yb5dVxd+fmR3AAMlrR1s2Izs/YxbVYnE6fOpnNxFwF0Lu5i4tTZThJtoNXXILaqDcWYX2tDNg5nxW6l59PDqFGSjlUaaH7mokWLmhqsmTXfpOlz6Fqy4jAcXUuWMWn6nIoispp2eVCuaMSywk6iImIyMBlgzJgxa0xHUtNmdTJp+hwWLO5i2OAOJowdxbjRHjnRbMHirpUqt9ZpdQ3i0VrTUX59LJfPZ8XhErdh+XCJazxXoc16Nmxwx0qVW+u0OkFcRhqBi/x6aV35Eflupt2Bp2pNUWsDV6HNejZh7Cg6Bg1Yoaxj0AAmjB1VUURW07QmJkkXAu8ChkiaD3wFOB2YIukY4CHg4Lz6H4D9SAPVPw98vFlxVcFVaLOe1Zpa3QTbfpqWICJifA+L9i5YN4DPNiuWqg0b3EFnQTJwFdosGTd6uBNCG/KT1C3gKrSZrYna5S6mtZqr0Ga2JnKCaBFXoc1sTeMmJjMzK+QEYWZmhZwgzMyskBOEmZkVcoIwM7NCThBmZlbICcLMzAr12+cg3P22mVnv+mWCqHW/Xethtdb9NuAkYWaW9csmJne/bWbWWL9MEO5+28yssX6ZIDyClZlZY/0yQbj7bTOzxvrlRWp3v21m1li/TBDg7rfNzBrpl01MZmbWmBOEmZkVcoIwM7NCThBmZlbICcLMzAo5QZiZWSEnCDMzK+QEYWZmhZwgzMyskBOEmZkVcoIwM7NCThBmZlao33bWZ2bWCtNmda6xPUdXUoOQ9HlJd0q6S9IXctnmkq6WdF9+3ayK2MzM+sq0WZ1MnDqbzsVdBNC5uIuJU2czbVZn1aGV0vIEIWln4JPAW4E3AR+QtD1wCjAjIrYHZuR5M7M11qTpc+hasmyFsq4ly5g0fU5FEa2cKmoQrwduiIjnI2IpcC1wALA/cF5e5zxgXAWxmZn1mZ7Gue+pvN1UkSDuBN4haQtJGwD7Aa8GtoqIhQD5dcuiN0s6VtJMSTMXLVrUsqDNzFZWT+Pc91TeblqeICLiHuA7wNXAVcDtwNKVeP/kiBgTEWOGDh3apCjNzFbfhLGj6Bg0YIWyjkEDmDB2VEURrZxKLlJHxDkRsVtEvAN4ArgPeFTS1gD59bEqYjMz6yvjRg/n2wfuwvDBHQgYPriDbx+4yxpzF1Mlt7lK2jIiHpO0LXAgsAcwEjgSOD2/XlpFbGZmfWnc6OFrTELorqrnIC6WtAWwBPhsRDwp6XRgiqRjgIeAgyuKzczMqChBRMTbC8oeB/auIBwzMyvQ8BqEpM/5oTUzs/6nzEXqVwE3S5oi6X2S1OygzMyseg0TREScCmwPnAMcBdwn6VuSXtvk2MzMLJs2q5O9Tv8LI0+5kr1O/0tLuusodZtrRATwSP5ZCmwG/F7SGU2MzczMqK5PpzLXII6XdAtwBvA/wC4R8RngzcCHmxqdmZlV1qdTmbuYhgAHRsSD9YUR8ZKkDzQnLDMzq6mqT6cyTUx/ID3tDICkjSW9DV7uNsPMzJqoqj6dyiSIHwPP1s0/l8vMzKwFqurTqUwTk/JFauDlpiWPRGdm1iK1rjpaPTJdmQP9A5KOZ3mt4T+AB5oXkpmZdVdFn05lmpg+DewJdALzgbcBxzYzKDMzq17DGkREPAYc2oJYzMysjTRMEJLWB44B3gCsXyuPiKObGJeZmVWsTBPTr0j9MY0ljR+9DfBMM4MyK6OKrgfM+pMyCeJ1EfFl4LmIOA94P7BLc8My611VXQ+Y9SdlEsSS/LpY0s7ApsCIpkVkVkJVXQ+Y9SdlbnOdnMeDOBW4DNgI+HJTozJroKquB8z6k14ThKR1gKcj4kngb8BrWhKVWQPDBnfQWZAMmt31gFl/0msTU0S8BHyuRbGYlVZV1wNm/UmZJqarJZ0EXETqhwmAiHii57eYNVdVXQ+Y9Seq62apeAVpbkFxRETlzU1jxoyJmTNnVh2GmdkaRdItETGm0XplnqQe2TchmZnZmqTMk9RHFJVHxPl9H46ZmbWLMtcg3lI3vT6wN3Ar4ARhZrYWK9PEdFz9vKRNSd1vmJnZWqzMk9TdPQ9s39eBmJlZeylzDeJyoHar0zrATsCUZgZlZmbVK3MN4r/qppcCD0bE/CbFY2ZmbaJMgngIWBgRLwBI6pA0IiLmNTUyMzOrVJlrEL8DXqqbX5bLzMxsLVYmQQyMiBdrM3l63eaFZGZm7aBMglgk6UO1GUn7A/9anZ1KOkHSXZLulHShpPUljZR0o6T7JF0kyUnIzKxCZRLEp4EvSXpI0kPAycCnVnWHkoYDxwNjImJnYABwKPAd4HsRsT3wJGkcbDMzq0jDBBER/xsRu5Nub31DROwZEfev5n4HAh2SBgIbAAuB9wC/z8vPA8at5j7MzGw1NEwQkr4laXBEPBsRz0jaTNI3V3WHEdFJunX2IVJieAq4BVgcEUvzavMB99tsZlahMk1M+0bE4tpMHl1uv1XdYR6+dH9gJDAM2BDYt2DVwn7IJR0raaakmYsWLVrVMMzMrIEyCWKApPVqM5I6gPV6Wb+RfYC5EbEoIpYAU4E9gcG5yQlgG2BB0ZsjYnJEjImIMUOHDl2NMMzMrDdlEsSvgRmSjpF0NHA1q9eT60PA7pI2kCRS77B3A38FDsrrHAlcuhr7MDOz1VSmN9czJN1BOvMX8I2ImL6qO4yIGyX9ntRl+FJgFjAZuBL4bb6+MQs4Z1X3YWZmq6/hkKOveIO0F/DRiPhsc0Iqz0OOmpmtvD4bcjRvbFdgPHAIMJd03cDMzNZiPSYISTuQHmAbDzwOXESqcby7RbGZmVmFeqtB3AtcB3yw9mCcpBNaEpWZmVWut7uYPgw8AvxV0s8k7U26SG1mZv1AjwkiIi6JiEOAHYFrgBOArST9WNJ7WxSfmZlVpExfTM9FxG8i4gOkB9huA05pemRmZlapMg/KvSwinoiIn0bEe5oVkJmZtYeVShBmZtZ/OEGYmVmhMt19byhpnTy9g6QPSRrU/NDMzKxKZZ6k/hvw9txN9wxgJumJ6sOaGZiZ2cqYNquTSdPnsGBxF8MGdzBh7CjGjfawMqujTBOTIuJ54EDghxFxAGl0OTOztjBtVicTp86mc3EXAXQu7mLi1NlMm9VZdWhrtFIJQtIepBrDlbmsVB9OZmatMGn6HLqWLFuhrGvJMiZNn1NRRGuHMgniC8BE4JKIuEvSa0hjN5iZtYUFi7tWqtzKKTMexLXAtZI2zPMPAMc3OzAzs7KGDe6gsyAZDBvcUUE0a48ydzHtIelu4J48/yZJZzc9MjOzkiaMHUXHoAErlHUMGsCEsaMqimjtUKaJ6fvAWFKX30TE7cA7mhmUmdnKGDd6ON8+cBeGD+5AwPDBHXz7wF18F9NqKnWxOSIeTsNHv2xZT+uamVVh3OjhTgh9rEyCeFjSnkBIWpd0/eGe5oZlZmZVK9PE9Gngs8BwYD6wa543M7O1WJkaxEsR4aemzcz6mTI1iBsl/U7Svup2IcLMzNZeZRLEDsBk4AjgfknfkrRDc8MyM7OqlRlRLiLi6ogYD3wCOBK4SdK1uQsOMzNbCzW8BiFpC+Bw4GPAo8BxwGWki9W/A0Y2M0AzM6tGmYvU1wO/AsZFxPy68pmSftKcsMzMrGplEsSoiIiiBRHxnT6Ox8zM2kSZBDFE0heBNwDr1woj4j1Ni8rMzCpX5i6m3wD3kq41fA2YB9zcxJjMzKwNlEkQW0TEOcCSiLg2Io4Gdm9yXGZmVrEyTUxL8utCSe8HFgDbNC8kMzNrB2USxDclbQqcCPwQ2AQ4oalRmZlZ5cqMKHdFnnwKePfq7lDSKOCiuqLXAKcB5+fyEaTrHB+JiCdXd39mZrZqerwGIWl9SUdK+pCSkyVdIekHkoas6g4jYk5E7BoRuwJvBp4HLgFOAWZExPbAjDxvZmYV6e0i9fnAe4GjgWuAbYGzgGeAc/to/3sD/xsRDwL7A+fl8vOAcX20DzMzWwW9NTHtFBE7SxoIzI+Id+byqyTd3kf7PxS4ME9vFRELASJioaQti94g6VjgWIBtt922j8IwM7PueqtBvAgQEUtJdy7VW+0hR/PodB8i9edUWkRMjogxETFm6NChqxuGmZn1oLcaxDaSzgRUN02e74uBX/cFbo2IR/P8o5K2zrWHrYHH+mAfZma2inpLEBPqpmd2W9Z9flWMZ3nzEqQeYo8ETs+vl/bBPszMbBX1mCAi4ryelq0uSRsA/w58qq74dGCKpGOAh4CDm7V/MzNrrMyDcn0uIp4HtuhW9jjpriYzM2sDZfpiMjOzfsgJwszMCjVMEJJ2kDRD0p15/o2STm1+aGZmVqUyNYifARPJvbpGxB2kB9zMzGwtViZBbBARN3UrW9qMYMzMrH2USRD/kvRaIAAkHQQsbGpUZmZWuTK3uX4WmAzsKKkTmAsc1tSozMyscmUSxIMRsY+kDYF1IuKZZgdlZmbVK9PENFfSZNI41M82OR4zM2sTZRLEKODPpKamuZLOkvRvzQ3LzMyq1jBBRERXREyJiAOB0aQxqa9temRmZlapUk9SS3qnpLOBW4H1gY80NSozM6tcw4vUkuYCtwFTgAkR8VzTozIzs8qVuYvpTRHxdNMjMTOzttJjgpD0xYg4A/impFcsj4jjmxmYmZlVq7caxD359ZZWBGJmZu2ltxHlLs+vTRtZzszM2leZi9RDgZOBnUh3MAEQEe9pYlxmZlaxMre5/obU3DQS+BowD7i5iTGZmVkbKJMgtoiIc4AlEXFtRBxN6nbDzMzWYmVuc12SXxdKej+wANimeSGZmVk7KJMgvilpU+BE4IekrjZOaGpUZmZWuYYJIiKuyJNPAe9ubjhmZtYuentQ7rRe3hcR8Y0mxGNmZm2itxpEUZ9LGwLHAFsAThC2yqbN6mTS9DksWNzFsMEdTBg7inGjh1cdlpnV6e1Bue/WpiVtDHwe+DjwW+C7Pb3PrJFpszqZOHU2XUuWAdC5uIuJU2cDOEmYtZFeb3OVtLmkbwJ3kJLJbhFxckQ81pLobK00afqcl5NDTdeSZUyaPqeiiMysSG/XICYBBwKTgV0iwsONWp9YsLhrpcrNrBq91SBOBIYBpwILJD2df56R5O6/bZUNG9yxUuVmVo0eE0RErBMRHRGxcURsUvezcURs0sogbe0yYewoOgYNWKGsY9AAJowdVVFEZlakzINyZn2qdiHadzGZtbdKEoSkwcDPgZ2BAI4G5gAXASNIHQJ+JCKerCI+a75xo4c7IZi1uTKd9TXDD4CrImJH4E2k3mJPAWZExPbAjDxvZmYVaXmCkLQJ8A7gHICIeDEiFgP7A7XBic4DxrU6NjMzW66KGsRrgEXALyXNkvRzSRsCW0XEQoD8umXRmyUdK2mmpJmLFi1qXdRmZv1MFQliILAb8OOIGE3q0qN0c1JETI6IMRExZujQoc2K0cys36siQcwH5kfEjXn+96SE8aikrQHyq5/WNjOrUMsTREQ8AjwsqXbT+97A3cBlwJG57Ejg0lbHZmZmy1X1HMRxwG8krQs8QOoEcB1giqRjgIeAgyuKzczMqChBRMRtwJiCRXu3OhYzMytW1XMQZmbW5pwgzMyskBOEmZkVcoIwM7NCThBmZlbICcLMzAo5QZiZWSEnCDMzK+QEYWZmhZwgzMyskBOEmZkVcoIwM7NCThBmZlbICcLMzAo5QZiZWSEnCDMzK+QEYWZmhZwgzMyskBOEmZkVcoIwM7NCThBmZlbICcLMzAo5QZiZWSEnCDMzK+QEYWZmhZwgzMyskBOEmZkVcoIwM7NCThBmZlZoYNUBmNnaY9qsTiZNn8OCxV0MG9zBhLGjGDd6eNVh2SpygjCzPjFtVicTp86ma8kyADoXdzFx6mwAJ4k1VCVNTJLmSZot6TZJM3PZ5pKulnRfft2sitjMbNVMmj7n5eRQ07VkGZOmz6koIltdVV6DeHdE7BoRY/L8KcCMiNgemJHnzWwNsWBx10qVW/trp4vU+wPn5enzgHEVxmJmK2nY4I6VKrf2V1WCCOBPkm6RdGwu2yoiFgLk1y2L3ijpWEkzJc1ctGhRi8I1s0YmjB1Fx6ABK5R1DBrAhLGjKorIVldVF6n3iogFkrYErpZ0b9k3RsRkYDLAmDFjolkBmtnKqV2I9l1Ma49KEkRELMivj0m6BHgr8KikrSNioaStgceqiM3MVt240cOdENYiLW9ikrShpI1r08B7gTuBy4Aj82pHApe2OjYzM1uuihrEVsAlkmr7vyAirpJ0MzBF0jHAQ8DBFcRmZmZZyxNERDwAvKmg/HFg71bHY2ZmxdrpNlczM2sjThBmZlZIEWvunaKSFgEPtmBXQ4B/tWA/q8tx9i3H2bfWhDjXhBhh9ePcLiKGNlppjU4QrSJpZl2XIG3LcfYtx9m31oQ414QYoXVxuonJzMwKOUGYmVkhJ4hyJlcdQEmOs285zr61JsS5JsQILYrT1yDMzKyQaxBmZlbICcLMzAo5QdSR9D5JcyTdL6nHEe0kHSQpJFVyO1yZOCV9RNLdku6SdEGrY8wx9BqnpG0l/VXSLEl3SNqvghh/IekxSXf2sFySzsyf4Q5Ju7U6xhxHozgPy/HdIekfkl7RnU0rNIqzbr23SFom6aBWxdZt/w3jlPSuPCzyXZKubWV8dTE0+r1vKulySbfnOD/epwFEhH/SdZgBwP8CrwHWBW4HdipYb2Pgb8ANwJh2jBPYHpgFbJbnt2zTOCcDn8nTOwHzKojzHcBuwJ09LN8P+CMgYHfgxlbHWDLOPet+3/u2a5x1fxt/Af4AHNSOcQKDgbuBbfN8y/+HSsb5JeA7eXoo8ASwbl/t3zWI5d4K3B8RD0TEi8BvScOgdvcN4AzghVYGV6dMnJ8EfhQRT0Iad6PFMUK5OAPYJE9vCixoYXwpgIi/kf6perI/cH4kNwCD83glLdUozoj4R+33TTp52aYlgb0yjkbfJ8BxwMVUOOZLiTg/CkyNiIfy+pXEWiLOADZW6h57o7zu0r7avxPEcsOBh+vm5+eyl0kaDbw6Iq5oZWDdNIwT2AHYQdL/SLpB0vtaFt1yZeL8KnC4pPmks8njWhPaSinzOdrNMaRaT9uRNBw4APhJ1bE0sAOwmaRr8tDIR1QdUA/OAl5POrmaDXw+Il7qq41XNeRoO1JB2cv3AEtaB/gecFSrAupBr3FmA0nNTO8inUleJ2nniFjc5NjqlYlzPHBuRHxX0h7Ar3KcffYH3gfKfI62IendpATxb1XH0oPvAydHxLI8Jky7Ggi8mTQEQQdwvaQbIuKf1Yb1CmOB24D3AK8lDeF8XUQ83Rcbdw1iufnAq+vmt2HFJo+NgZ2BayTNI7VHX1bBhepGcdbWuTQilkTEXGAOKWG0Upk4jwGmAETE9cD6pE7I2kmZz9EWJL0R+Dmwf6TxVdrRGOC3+X/oIOBsSeOqDanQfOCqiHguIv5Fuu5YyYX/Bj5OagqLiLgfmAvs2Fcbd4JY7mZge0kjJa0LHEoaBhWAiHgqIoZExIiIGEFq5/1QRMxspzizacC7ASQNIVWXH2hplOXifIg8SJSk15MSxKKWRtnYZcAR+W6m3YGnImJh1UF1J2lbYCrwsTY8y31ZRIys+x/6PfAfETGt4rCKXAq8XdJASRsAbwPuqTimIvX/Q1sBo+jD/3U3MWURsVTS54DppLssfhERd0n6OjAzIrof3CpRMs7pwHsl3Q0sAya0+oyyZJwnAj+TdAKp2eaoyLdjtIqkC0lNcUPytZCvAIPyZ/gJ6drIfsD9wPOkM7aWKxHnacAWpDNygKVRQa+kJeJsC40h1GZoAAAGy0lEQVTijIh7JF0F3AG8BPw8Inq9dbeKOEk3zZwraTapOfTkXOPpm/23+P/RzMzWEG5iMjOzQk4QZmZWyAnCzMwKOUGYmVkhJwgzMyvkBNHPSTog90zbZw/XNEvuXfOKuvn3SbpJ0r25182L8vMA3d937sr0GipphKSP9lXcq0vSpZKu71Y2TtJOdfNHSRrWyza+LmmfPD0vPx9Tdv+r9H3k3lpvk3SnpN/l5wl6W/8Pkgav7H6seZwgbDzwd9KDbKtN0oC+2E6J/ewM/BA4MiJ2jIhdgd8AI/pg8yNInbVVLh8wdyN1EjiybtE4Ug+4NUcBhQlC0oCIOC0i/ryKYYxg1b6ProjYNSJ2Bl4EPt3byhGxX/fuYPIDij5OVcRffD8maSNgL1KXF4fWlV+kurEZ8hn4hyUNkDRJ0s1K4w58Ki9/l9K4DheQOgxD0rTcydldko6t29Yxkv6ZO0H7maSzcvlQSRfnbd8saa8G4Z8MfCsiXn66NSIuy71fFtlH0nV53x/I+yz8PMDppKdob5N0Qj6zfWN+zyxJp+Xpb0j6RJ6eULedr9V93sNzLec2ST+tJVBJz0r6T6V+/G/IT8EW+TBwOak33EPze/cEPgRMyts9mdSFxW/yfEeuJZwm6e/AwQW1qAk5rpskvS5vd4V1JD3bw/fR0/fWm+uA2n56+tuYJ2lIrrHcI+ls4Fbg1Tm2OyXNVnqw0lqhr/oN98+a9wMcDpyTp/8B7JanDwDOy9Prknoz7QCOBU7N5esBM4GRpCc9nwNG1m178/zaAdxJesp3GDAP2Jz0NOh1wFl5vQuAf8vT2wL3FMT7LuCKPH0r8KaSn/Nc4CrSCdH2pH521m/wea6oe/8pwGdJXZPfDEzP5X8ldW3wXtLYFsr7uILUj//rSQf3QXn9s4Ej8nQAH8zTZ9TiKIj9z8DbSd2l3NHtMx1UN38NdeOT5O/5i0Xr52X/L08fUfeddt/ms92/9zxf+L0VxF57/0BS1xW1sT9e8bdRF9cQUo3lJWD3XP5m4Oq67Q6u+n+nv/y4q43+bTypd01IZ6jjSQfePwJnSloPeB/wt4jokvRe4I11Z5mbkg64LwI3ReoYsOZ4SQfk6Vfn9V4FXBsRTwBI+h3pwAewD7CTlvfwuYmkjSPimUYfQtIWwAxgA2ByRPxXwWpTIvUSe5+kB0gdmvX2eepdBxxP6gjtSuDfc3v6iIiYI+mTeVuz8vob5e28kXRwuzl/rg6Wj4HwIimRANwC/HvB59qKdNb994gISUuVerst2+XDRb0su7Du9Xslt1fT0/c2t9t6HZJuy9PXAefk6aK/je5dwTwYafwNSH0LvUbSD0nf/59WMl5bRU4Q/VQ+qL4H2FlSkPpLCklfjIgXJF1D6kr4EJYfTAQcFxHTu23rXaQaRP38PsAeEfF83tb6FHedXbNOXr+r5Ee4i9Q2f3ukfqZ2lXQS6eBcpHufMtHg89S7mdSE8wBwNeks95OkAzt5O9+OiJ92285xpJrYxIJ4lkQ+HSb1l1X0v3gIsBkwNyeYTUjNTKcWf8RXeK6XZVEwvZTc7Ky0w3V7eG/h91agK9K1oeVv7Plvo8fYI+JJpSFUx5Jqch8Bjm6wb+sDvgbRfx1EGiltu0i9a76adAZYG0fgt6SO6d5O6nCP/PoZSYMAJO0gacOCbW8KPJkPADuSukYHuAl4p6TNJA0kta/X/An4XG1G0goHlgJnAP9PqRfYmt7ukjlY0jqSXksaBnVOL5/nGVL37gBEGhHvYdKB6QbS2fBJ+ZW8naOVrukgabikLUm1moPyNJI2l7Rdg89Vbzzwvlje++mbWX6taIUYC+YbOaTutXaH1Ly8D0gj6Q3qYdtl/w6K9PS30SOlO67WiYiLgS+TTgysBVyD6L/Gky4+1ruYdLfKdaQD9vnAZfkACWmsgRHArfkMcxHpbprurgI+LekO0oH4BoCI6JT0LeBG0pgKdwNP5fccD/wov2cgqf/9Hu96iYjZkj4PnC9pY1ITxUOk3i6LzAGuBbYCPp1rST19njuApZJuJw1o9L38neydD2zXkQdiyrH8KSeq6/OZ/rPA4RFxt6RTgT8p3YmzhHQG/GBPn6tG0gjStZhaMwsRMVfS05LeRkrgP5N0PCnZnwv8RFIXsEej7QPrSbqRdJI4Ppf9DLhU0k2k5FY7i1/h+wB+0MP3Vkbh30YDw4FfavndTEU1MmsC9+ZqLSVpo4h4NtcgLiF1A35J1XGZ2Su5icla7av5wuWdpCatdhwsxsxwDcLMzHrgGoSZmRVygjAzs0JOEGZmVsgJwszMCjlBmJlZof8P36Y+PpS23O0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.165815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.165815</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0  1.000000 -0.165815\n",
       "1 -0.165815  1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mapper = defaultdict(list)\n",
    "for i in ig.keys():\n",
    "    mapper[i].append(ig[i])\n",
    "    mapper[i].append(acc[i])\n",
    "mapper.values()\n",
    "plt.scatter([i[0] for i in mapper.values()], [i[1] for i in mapper.values()])\n",
    "plt.title(\"Naive Bayes Assumption Proof\")\n",
    "plt.xlabel(\"Average IG between Attribute Pairs\")\n",
    "plt.ylabel(\"Naive Bayes Accuracy\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "display(DataFrame.from_dict(mapper).T.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Questions 1, 2, 4 and 6 (150 - 200 words for each response):\n",
    "\n",
    "#### 1. The Naive Bayes classifiers can be seen to vary, in terms of their effectiveness on the given datasets (e.g. in terms of Accuracy). Consider the Information Gain of each attribute, relative to the class distribution — does this help to explain the classifiers’ behaviour? Identify any results that are particularly surprising, and explain why they occur.\n",
    "    - According to the Information Gain for each attribute given a class, we find that if there is ONE attribute with SIGNIFICANTLY MORE Information Gain COMPARED to every other attribute, we result in an overall HIGHER ACCURACY\n",
    "    - Datasets that have attributes with SIMILAR Information Gain with the class label will not perform as well as other datasets\n",
    "        - Can be seen with the `primary-tumor.csv` where all the information gain is roughly between 0.1 - 0.2\n",
    "        - Compare that to `mushroom.csv` where there is one attribute with 0.9 information gain, and the rest between 0.1 - 0.4 and some less than 0.1\n",
    "    - There are exceptions however - note 'hypothyroid.csv' whose attributes are very little information gain despite high Naive Bayes classification errors.y i dunno \n",
    "    \n",
    "\n",
    "#### 2. The Information Gain can be seen as a kind of correlation coefficient between a pair of attributes: when the gain is low, the attribute values are uncorrelated; when the gain is high, the attribute values are correlated. In supervised ML, we typically calculate the Infomation Gain between a single attribute and the class, but it can be calculated for any pair of attributes. Using the pair-wise IG as a proxy for attribute interdependence, in which cases are our NB assumptions violated? Describe any evidence (or indeed, lack of evidence) that this is has some effect on the effectiveness of the NB classifier.\n",
    "In any given dataset, two attributes may depend on each other, but the dependence may be distributed evenly within the attributes. In this case, we can see that the Naive Bayes's assumption of conditional independence is violated. If we just look at the two attributes, there may be a strong dependence between them that can affect the classification. However, if you take into account the dependencies when classifying all together, they may cancel each other out and not affect the classification. From this, we can argue that it is the distribution of the attribute dependencies, rather than the dependencies between attributes (pair-wise IG) that affect the Naive Bayes Classification (correlation does not imply causation, there may be an underlying distribution). If you look at the graph above, we can also see that across all the datasets, there seems to no evidence that the Information Gain between attributes plays a role in affecting accuracy. If anything, a look at the correlation table suggests that there is a **Weak** relationship between pair-wise IG and the performance of our model (in terms of accuracy). As such, there is a lack of evidence that it has some effect on the effectiveness of the Naive Bayes classifier.\n",
    "    \n",
    "\n",
    "#### 4. Evaluating the model on the same data that we use to train the model is considered to be a major mistake in Machine Learning. Implement a hold–out or cross–validation evaluation strategy. How does your estimate of effectiveness change, compared to testing on the training data? Explain why. (The result might surprise you!)\n",
    "Implementation: $k$-Fold Cross-Validation\n",
    "    \n",
    "In theory, we should expect a higher accuracy if more data is available, since there is more information for the model to use. This means that testing our model on the training data would be the *best case scenario* for accuracy, since our supervised model has seen everything prior to testing. If we take this into account, then our $k$-Fold Cross Validation does a reasonable job (typically a 2% difference or even equal performance in accuracy using $k$ = 10) despite being partitioned.  The reason as to why $k$-Fold Cross Validation seems to perform better than a Hold-Out strategy can be accounted by:\n",
    "- The *random noise* which $k$-Fold Cross Validation can successfully reduce since it is averaging over the $k$ sets of estimations\n",
    "- Unlike Hold-Out, where a split of the data is used, the fact that every training instance is eventually used as a testing instance in a partition means that our model is not missing out on any information overall\n",
    "    \n",
    "\n",
    "#### 6. Naive Bayes is said to elegantly handle missing attribute values. For the datasets with missing values, is there any evidence that the performance is different on the instances with missing values, compared to the instances where all of the values are present? Does it matter which, or how many values are missing? Would an imputation strategy have any effect on this?\n",
    "    - To an extent, it does matter how many values are missing, but it is also dependent if the attribute has a high information gain or not\n",
    "    - Missing values in itself could be siginficiant. i.e. Since there is a missing value, it could lead to a specific label class\n",
    "    - See table below for imputation impact on testing on the training data and 10-fold cross validation\n",
    "        - Can see that the training has no effect whether we impute values or not (makes sense since the model is supervised and has therefore seen the solution before)\n",
    "        - Imputing the missing values and then testing on imputed values is the same as skipping the missing values in training, and then skipping missing values in testing\n",
    "        - However, we can see that the *overall accuracy increases slightly* when testing using the 10-fold cross-validation method\n",
    "    \n",
    "    \n",
    "To an extent, it does not matter how many values are missing, but more dependent if the attribute with the missing values has a high information gain or not. This may be since missing values in itself could be significant (i.e. because there is a missing value, it could lead to a specific class label). For datasets with missing values, we used a mode imputation to fill in missing values, although from the results in the table below, we can see that there is no signficiant impact of imputing values. \n",
    "The only increase in accuracy is when testing using the $k$-Fold Cross-Validation method which may be due to its ability in reducing random noise.\n",
    "This backs the claim that the Naive Bayes is able to elegantly handle missing attribute values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 6 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No Imputations and No dropping 0 info gain</th>\n",
       "      <th>10 fold Cross Validation result 1</th>\n",
       "      <th>No Imputations and Dropping 0 info gain</th>\n",
       "      <th>10 fold Cross Validation result 2</th>\n",
       "      <th>Mode Imputations and No dropping 0 info gain</th>\n",
       "      <th>10 fold Cross Validation result 3</th>\n",
       "      <th>Mode Imputations and Dropping 0 info gain</th>\n",
       "      <th>10 fold Cross Validation result 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99.11</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.11</td>\n",
       "      <td>98.99</td>\n",
       "      <td>99.11</td>\n",
       "      <td>98.78</td>\n",
       "      <td>99.11</td>\n",
       "      <td>99.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75.52</td>\n",
       "      <td>71.29</td>\n",
       "      <td>75.52</td>\n",
       "      <td>72.77</td>\n",
       "      <td>75.85</td>\n",
       "      <td>72.01</td>\n",
       "      <td>75.87</td>\n",
       "      <td>71.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87.38</td>\n",
       "      <td>85.42</td>\n",
       "      <td>87.38</td>\n",
       "      <td>85.71</td>\n",
       "      <td>87.38</td>\n",
       "      <td>85.82</td>\n",
       "      <td>87.38</td>\n",
       "      <td>85.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50.58</td>\n",
       "      <td>49.08</td>\n",
       "      <td>50.58</td>\n",
       "      <td>49.70</td>\n",
       "      <td>50.58</td>\n",
       "      <td>49.15</td>\n",
       "      <td>50.58</td>\n",
       "      <td>49.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85.16</td>\n",
       "      <td>83.71</td>\n",
       "      <td>85.16</td>\n",
       "      <td>83.17</td>\n",
       "      <td>84.52</td>\n",
       "      <td>84.50</td>\n",
       "      <td>84.52</td>\n",
       "      <td>84.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>95.23</td>\n",
       "      <td>95.23</td>\n",
       "      <td>95.23</td>\n",
       "      <td>95.23</td>\n",
       "      <td>95.23</td>\n",
       "      <td>95.23</td>\n",
       "      <td>95.23</td>\n",
       "      <td>95.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>99.72</td>\n",
       "      <td>99.69</td>\n",
       "      <td>99.72</td>\n",
       "      <td>99.68</td>\n",
       "      <td>99.58</td>\n",
       "      <td>99.68</td>\n",
       "      <td>99.58</td>\n",
       "      <td>99.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>90.31</td>\n",
       "      <td>90.32</td>\n",
       "      <td>90.31</td>\n",
       "      <td>90.25</td>\n",
       "      <td>90.31</td>\n",
       "      <td>90.33</td>\n",
       "      <td>90.31</td>\n",
       "      <td>90.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>61.65</td>\n",
       "      <td>46.27</td>\n",
       "      <td>61.65</td>\n",
       "      <td>46.31</td>\n",
       "      <td>57.52</td>\n",
       "      <td>46.90</td>\n",
       "      <td>57.52</td>\n",
       "      <td>48.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   No Imputations and No dropping 0 info gain  \\\n",
       "0                                       99.11   \n",
       "1                                       75.52   \n",
       "2                                       87.38   \n",
       "3                                       50.58   \n",
       "4                                       85.16   \n",
       "5                                       95.23   \n",
       "6                                       99.72   \n",
       "7                                       90.31   \n",
       "8                                       61.65   \n",
       "\n",
       "   10 fold Cross Validation result 1  No Imputations and Dropping 0 info gain  \\\n",
       "0                              99.00                                    99.11   \n",
       "1                              71.29                                    75.52   \n",
       "2                              85.42                                    87.38   \n",
       "3                              49.08                                    50.58   \n",
       "4                              83.71                                    85.16   \n",
       "5                              95.23                                    95.23   \n",
       "6                              99.69                                    99.72   \n",
       "7                              90.32                                    90.31   \n",
       "8                              46.27                                    61.65   \n",
       "\n",
       "   10 fold Cross Validation result 2  \\\n",
       "0                              98.99   \n",
       "1                              72.77   \n",
       "2                              85.71   \n",
       "3                              49.70   \n",
       "4                              83.17   \n",
       "5                              95.23   \n",
       "6                              99.68   \n",
       "7                              90.25   \n",
       "8                              46.31   \n",
       "\n",
       "   Mode Imputations and No dropping 0 info gain  \\\n",
       "0                                         99.11   \n",
       "1                                         75.85   \n",
       "2                                         87.38   \n",
       "3                                         50.58   \n",
       "4                                         84.52   \n",
       "5                                         95.23   \n",
       "6                                         99.58   \n",
       "7                                         90.31   \n",
       "8                                         57.52   \n",
       "\n",
       "   10 fold Cross Validation result 3  \\\n",
       "0                              98.78   \n",
       "1                              72.01   \n",
       "2                              85.82   \n",
       "3                              49.15   \n",
       "4                              84.50   \n",
       "5                              95.23   \n",
       "6                              99.68   \n",
       "7                              90.33   \n",
       "8                              46.90   \n",
       "\n",
       "   Mode Imputations and Dropping 0 info gain  \\\n",
       "0                                      99.11   \n",
       "1                                      75.87   \n",
       "2                                      87.38   \n",
       "3                                      50.58   \n",
       "4                                      84.52   \n",
       "5                                      95.23   \n",
       "6                                      99.58   \n",
       "7                                      90.31   \n",
       "8                                      57.52   \n",
       "\n",
       "   10 fold Cross Validation result 4  \n",
       "0                              99.00  \n",
       "1                              71.59  \n",
       "2                              85.07  \n",
       "3                              49.15  \n",
       "4                              84.00  \n",
       "5                              95.23  \n",
       "6                              99.68  \n",
       "7                              90.37  \n",
       "8                              48.40  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "display(read_csv('results.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
