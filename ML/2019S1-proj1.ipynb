{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2019 Semester 1\n",
    "-----\n",
    "## Project 1: Gaining Information about Naive Bayes\n",
    "-----\n",
    "###### Student Name(s): Akira and Callum\n",
    "###### Python version: 3.7.1 from Anaconda \n",
    "###### Submission deadline: 1pm, Fri 5 Apr 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you may use for your Project 1 submission. (You are not required to use it; in particular, there is no need to use iPython if you do not like it.)\n",
    "\n",
    "Marking will be applied on the five functions that are defined in this notebook, and to your responses to the questions at the end of this notebook.\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from collections import *\n",
    "import numpy as np\n",
    "from math import log\n",
    "\n",
    "########## POSSIBLE CSVs ##########\n",
    "d1 =  'anneal.csv'\n",
    "h1 = 'family,product-type,steel,carbon,hardness,temper_rolling,condition,formability,strength,non-ageing,surface-finish,surface-quality,enamelability,bc,bf,bt,bw-me,bl,m,chrom,phos,cbond,marvi,exptl,ferro,corr,bbvc,lustre,jurofm,s,p,shape,oil,bore,packing,class'.split(',')\n",
    "\n",
    "d2 =  'breast-cancer.csv'\n",
    "h2 = 'age,menopause,tumor-size,inv-nodes,node-caps,deg-malig,breast,breast-quad,irradiat,class'.split(',')\n",
    "\n",
    "d3 =  'car.csv'\n",
    "h3 = 'buying,maint,doors,persons,lug_boot,safety,class'.split(',')\n",
    "\n",
    "d4 =  'cmc.csv'\n",
    "h4 = 'w-education,h-education,n-child,w-relation,w-work,h-occupation,standard-of-living,media-exposure,class'.split(',')\n",
    "\n",
    "d5 =  'hepatitis.csv'\n",
    "h5 = 'sex,steroid,antivirals,fatigue,malaise,anorexia,liver-big,liver-firm,spleen-palpable,spiders,ascites,varices,histology,class'.split(',')\n",
    "\n",
    "d6 =  'hypothyroid.csv'\n",
    "h6 = 'sex,on-thyroxine,query-on-thyroxine,on_antithyroid,surgery,query-hypothyroid,query-hyperthyroid,pregnant,sick,tumor,lithium,goitre,TSH,T3,TT4,T4U,FTI,TBG,class'.split(',')\n",
    "\n",
    "d7 =  'mushroom.csv'\n",
    "h7 = 'cap-shape,cap-surface,cap-color,bruises,odor,gill-attachment,gill-spacing,gill-size,gill-color,stalk-shape,stalk-root,stalk-surface-above-ring,stalk-surface-below-ring,stalk-color-above-ring,stalk-color-below-ring,veil-type,veil-color,ring-number,ring-type,spore-print-color,population,habitat,class'.split(',')\n",
    "\n",
    "d8 =  'nursery.csv'\n",
    "h8 = 'parents,has_nurs,form,children,housing,finance,social,health,class'.split(',')\n",
    "\n",
    "d9 = 'primary-tumor.csv'\n",
    "h9 = 'age,sex,histologic-type,degree-of-diffe,bone,bone-marrow,lung,pleura,peritoneum,liver,brain,skin,neck,supraclavicular,axillar,mediastinum,abdominal,class'.split(',')\n",
    "\n",
    "datasets = [d1,d2,d3,d4,d5,d6,d7,d8,d9]\n",
    "dataset_headers = [h1,h2,h3,h4,h5,h6,h7,h8,h9]\n",
    "\n",
    "dictionary = {datasets[i] : dataset_headers[i] for i in range(len(datasets))}\n",
    "\n",
    "def set_column(filename):\n",
    "    return dictionary[filename]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Has been adjusted so that it works with testing on the train data, and partitioning for cross_val\"\"\"\n",
    "def preprocess(filename, testing_on_train = True, k = 10, drop = 'no'):\n",
    "    # Add column headers, drop columns with only one unique value (all same value)\n",
    "    df = pd.read_csv(filename, header = None, names = set_column(filename))\n",
    "    \n",
    "    df.replace('?', np.NaN, inplace=True)\n",
    "    mode = df.mode().iloc[0]\n",
    "    df = df.fillna(mode)\n",
    "    \n",
    "    \"\"\"If we are not using the cross validation method\"\"\"\n",
    "    # Return the whole dataset as a dataframe\n",
    "    if testing_on_train:\n",
    "        return df\n",
    "    else:\n",
    "        \"\"\"Drop no gain attributes given the input in the script\"\"\"\n",
    "        if drop in 'yesYesYES':\n",
    "            non_unique = df.apply(pd.Series.nunique)\n",
    "            df.drop(non_unique[non_unique == 1].index, axis=1, inplace=True)\n",
    "            \n",
    "        temp = df.copy()\n",
    "        partitions = list()\n",
    "        \n",
    "        # k-fold Cross Validation\n",
    "        divisor = k\n",
    "        \n",
    "        for i in range(k):\n",
    "            partitions.append(temp.sample(frac=1/divisor))\n",
    "            divisor -= 1\n",
    "            temp.drop(partitions[-1].index, axis=0, inplace=True)\n",
    "        \n",
    "        del temp\n",
    "        \n",
    "        # Dictionary of train/test pairs\n",
    "        cross_validation_pairs = defaultdict(list)\n",
    "        models = list()\n",
    "        \n",
    "        for i in range(k):\n",
    "            test = partitions[i]\n",
    "            train = df.iloc[df.index.drop(test.index.values)]\n",
    "            cross_validation_pairs[\"train\"].append(train)\n",
    "            cross_validation_pairs[\"test\"].append(test)\n",
    "            \n",
    "        return cross_validation_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Trains a model given a training dataset\"\"\"\n",
    "def train(train_set):\n",
    "    N = len(train_set)\n",
    "    priors = {}\n",
    "    posteriors = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))\n",
    "    \"\"\"Accessable using posteriors[class j][attribute x][value i]\"\"\"\n",
    "    \n",
    "    for label in train_set['class'].unique():\n",
    "        priors[label] = len(train_set.loc[train_set['class'] == label]) / N\n",
    "        for attribute in train_set.columns[:-1]:\n",
    "            temp = train_set.loc[train_set['class'] == label, [attribute,'class']]\n",
    "            n = len(temp)\n",
    "            count = Counter(temp[attribute])\n",
    "            for i in count:\n",
    "                posteriors[label][attribute][i] = count[i] / n\n",
    "                \n",
    "    trained_model = {\"priors\": priors, \"posteriors\": posteriors}\n",
    "    \n",
    "    return trained_model\n",
    "\n",
    "\"\"\"Trains M partitions for cross_val\"\"\"\n",
    "def cross_validation_train(cross_validation_pairs):\n",
    "    trained_models = list()\n",
    "    train_set = cross_validation_pairs[\"train\"]\n",
    "    test_set = cross_validation_pairs[\"test\"]\n",
    "    N = len(train_set)\n",
    "    \n",
    "    for i in range(N):\n",
    "        trained_models.append(train(train_set[i]))\n",
    "        \n",
    "    return trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Predicts a test set\"\"\"\n",
    "def predict(trained_model, test_set):\n",
    "    priors = trained_model[\"priors\"]\n",
    "    posteriors = trained_model[\"posteriors\"]\n",
    "    \n",
    "    \"\"\"Drop the class labels of the test set\"\"\"\n",
    "    test_labels = test_set['class']\n",
    "    test = test_set.drop('class', axis=1)\n",
    "    cols = test_set.columns\n",
    "    \n",
    "    \"\"\"Probabilistic Smoothing with epsilon -> 0\"\"\"\n",
    "    n = len(test_labels)\n",
    "    epsilon = 1e-100\n",
    "    \n",
    "    \"\"\"Model Prediction\"\"\"\n",
    "    prediction = {}\n",
    "    \n",
    "    \"\"\"The Predicted Labels to be Returned\"\"\"\n",
    "    \"\"\"(Key, Value) = (Test Instance Row, Predicted Label)\"\"\"\n",
    "    predicted_labels = {}\n",
    "    \n",
    "    for i in range(n):\n",
    "        instance = test.iloc[i]\n",
    "        for label in priors.keys():       \n",
    "            prob = log(priors[label])/log(2)\n",
    "            for attribute in cols:\n",
    "                try:\n",
    "                    \"\"\"If the valyue is non missing\"\"\"\n",
    "                    if instance[attribute] != '?':\n",
    "                        prob += log(posteriors[label][attribute][instance[attribute]])/log(2)\n",
    "                    else:\n",
    "                        \"\"\"Otherwise we have chosen to simply ignore it\"\"\"\n",
    "                        pass\n",
    "                except:\n",
    "                    \"\"\"If the value does not exist in our model, we use epsilon\"\"\"\n",
    "                    prob += log(epsilon)/log(2)\n",
    "                    \n",
    "            prediction[label] = prob\n",
    "        \n",
    "        \"\"\"Choose the predicted class with the highest probability\"\"\"\n",
    "        predicted_labels[i] = max(prediction, key=prediction.get)\n",
    "        \n",
    "    return predicted_labels\n",
    "\n",
    "\"\"\"Tests partitions\"\"\"\n",
    "def cross_validation_predict(trained_models, cross_validation_pairs):\n",
    "    test_set = cross_validation_pairs[\"test\"]\n",
    "    N = len(test_set)\n",
    "    predictions = list()\n",
    "    \n",
    "    for i in range(N):\n",
    "        predictions.append(predict(trained_models[i], test_set[i]))\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluates the accuracy\"\"\"\n",
    "def evaluate(predicted_labels, test_set):\n",
    "    test_labels = test_set['class']\n",
    "    n = len(test_labels)\n",
    "    \n",
    "    return [1 if predicted_labels[i] == test_labels.iloc[i] else 0 for i in range(n)]\n",
    "\n",
    "\"\"\"Evaluates cross_val accuracy\"\"\"\n",
    "def cross_validation_evaluate(predictions, cross_pairs):\n",
    "    test = cross_pairs[\"test\"]\n",
    "    N = len(test)\n",
    "    results = list()\n",
    "    for i in range(N):\n",
    "        results.append(evaluate(predictions[i], test[i]))\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Calculates the entropy given a series\"\"\"\n",
    "def entropy(attribute_value):\n",
    "    # Calculates the probability of class given that it has been loc'd on a said value\n",
    "    event = pd.Series(attribute_value).value_counts(normalize=True, sort=False)\n",
    "    return -(event * [log(i) for i in event]/log(2)).sum()\n",
    "\n",
    "\"\"\"Calculates the mean information given a dataset\"\"\"\n",
    "def mean_info(dataset):\n",
    "    mean_info_per_attribute = defaultdict(float)\n",
    "    \n",
    "    for attribute in dataset.columns[:-1]:\n",
    "        # Calculates the probability of said value to happen (number of values / total number of instances)\n",
    "        value_probabilities = dataset[attribute].value_counts(normalize=True, sort=False).to_dict()\n",
    "        for value in dataset[attribute].unique():\n",
    "            # dataframe loc on said value and return the corresponding class column\n",
    "            corresponding_values = dataset.loc[dataset[attribute] == value, 'class']\n",
    "            # add probability of said value * entropy of said value to the attribute mean info\n",
    "            mean_info_per_attribute[attribute] += value_probabilities[value] * entropy(corresponding_values)\n",
    "            \n",
    "    return mean_info_per_attribute\n",
    "\n",
    "\"\"\"Calculates the information gain given a dataset. Adjusted so that it can drop 0 info_gain columns\"\"\"\n",
    "def info_gain(dataset, drop_no_gain = False):\n",
    "    mean_info_per_attribute = mean_info(dataset)\n",
    "    class_entropy = entropy(dataset['class'])\n",
    "    info_gain_given_class = defaultdict(float)\n",
    "    \n",
    "    for attribute in mean_info_per_attribute:\n",
    "\n",
    "        info_gain_given_class[attribute] = class_entropy - mean_info_per_attribute[attribute]\n",
    "    \n",
    "    \"\"\"If we want to drop the columns with absolutely 0 information gain\"\"\"\n",
    "    if drop_no_gain:\n",
    "        non_unique = dataset.apply(pd.Series.nunique)\n",
    "        dataset.drop(non_unique[non_unique == 1].index, axis=1, inplace=True)\n",
    "        return dataset\n",
    "    else:\n",
    "        return info_gain_given_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "Enter k value for k-Fold Cross Validation: 10\n",
      "Drop all columns with absolutely no information gain? (y/n): y\n",
      "Print the information gain? (y/n): y\n",
      "****************************************\n",
      "Processing anneal.csv ...\n",
      "TESTING ON THE TRAIN DATA\n",
      "InfoGain(family | class) = 0.40908953764450995\n",
      "InfoGain(steel | class) = 0.30605153542894037\n",
      "InfoGain(carbon | class) = 0.051344088764403883\n",
      "InfoGain(hardness | class) = 0.2910822058599468\n",
      "InfoGain(temper_rolling | class) = 0.1471188622809554\n",
      "InfoGain(condition | class) = 0.21372288031590858\n",
      "InfoGain(formability | class) = 0.29223544065798424\n",
      "InfoGain(strength | class) = 0.1261663361036094\n",
      "InfoGain(non-ageing | class) = 0.1410737916381286\n",
      "InfoGain(surface-finish | class) = 0.032488406491841815\n",
      "InfoGain(surface-quality | class) = 0.4351778362628853\n",
      "InfoGain(enamelability | class) = 0.03870173274881039\n",
      "InfoGain(bc | class) = 0.0004376065202116308\n",
      "InfoGain(bf | class) = 0.039355574142836636\n",
      "InfoGain(bt | class) = 0.021775078259213432\n",
      "InfoGain(bw-me | class) = 0.03799747881351134\n",
      "InfoGain(bl | class) = 0.03670308136440803\n",
      "InfoGain(chrom | class) = 0.11722522630372034\n",
      "InfoGain(phos | class) = 0.029753745208638716\n",
      "InfoGain(cbond | class) = 0.027042353328676327\n",
      "InfoGain(exptl | class) = 0.015604780443500221\n",
      "InfoGain(ferro | class) = 0.13718113252042552\n",
      "InfoGain(bbvc | class) = 0.02239708985164568\n",
      "InfoGain(lustre | class) = 0.01824168402125026\n",
      "InfoGain(shape | class) = 0.043239605565149386\n",
      "InfoGain(oil | class) = 0.03303757117705697\n",
      "InfoGain(bore | class) = 0.019378864328318812\n",
      "InfoGain(packing | class) = 0.003958783545891853\n",
      "Accuracy for Testing on the Training Data: 99.11%\n",
      "\n",
      "10-FOLD CROSS VALIDATION\n",
      "... ...\n",
      "Accuracy using k-Fold Cross Validation: 98.89% 96.67% 98.89% 98.89% 98.89% 97.78% 100.00% 100.00% 100.00% 100.00%\n",
      "Average 10-Fold Cross Validation Accuracy: 99.00%\n",
      "****************************************\n",
      "Processing breast-cancer.csv ...\n",
      "TESTING ON THE TRAIN DATA\n",
      "InfoGain(age | class) = 0.010605956535614136\n",
      "InfoGain(menopause | class) = 0.0020016149737116518\n",
      "InfoGain(tumor-size | class) = 0.05717112532429669\n",
      "InfoGain(inv-nodes | class) = 0.06899508808988597\n",
      "InfoGain(node-caps | class) = 0.05136145395409375\n",
      "InfoGain(deg-malig | class) = 0.07700985251661441\n",
      "InfoGain(breast | class) = 0.0024889884332652823\n",
      "InfoGain(breast-quad | class) = 0.009338656255899025\n",
      "InfoGain(irradiat | class) = 0.025819023909141037\n",
      "Accuracy for Testing on the Training Data: 75.87%\n",
      "\n",
      "10-FOLD CROSS VALIDATION\n",
      "... ...\n",
      "Accuracy using k-Fold Cross Validation: 82.76% 72.41% 92.86% 75.86% 78.57% 68.97% 67.86% 79.31% 67.86% 68.97%\n",
      "Average 10-Fold Cross Validation Accuracy: 75.54%\n",
      "****************************************\n",
      "Processing car.csv ...\n",
      "TESTING ON THE TRAIN DATA\n",
      "InfoGain(buying | class) = 0.09644896916961376\n",
      "InfoGain(maint | class) = 0.07370394692148574\n",
      "InfoGain(doors | class) = 0.004485716626631886\n",
      "InfoGain(persons | class) = 0.21966296333990798\n",
      "InfoGain(lug_boot | class) = 0.030008141247605202\n",
      "InfoGain(safety | class) = 0.26218435655426375\n",
      "Accuracy for Testing on the Training Data: 87.38%\n",
      "\n",
      "10-FOLD CROSS VALIDATION\n",
      "... ...\n",
      "Accuracy using k-Fold Cross Validation: 87.28% 86.71% 85.55% 90.17% 83.24% 84.39% 87.79% 86.13% 83.72% 84.97%\n",
      "Average 10-Fold Cross Validation Accuracy: 86.00%\n",
      "****************************************\n",
      "Processing cmc.csv ...\n",
      "TESTING ON THE TRAIN DATA\n",
      "InfoGain(w-education | class) = 0.07090633894894571\n",
      "InfoGain(h-education | class) = 0.0401385992293839\n",
      "InfoGain(n-child | class) = 0.10173991727554066\n",
      "InfoGain(w-relation | class) = 0.00982050143438462\n",
      "InfoGain(w-work | class) = 0.002582332379721386\n",
      "InfoGain(h-occupation | class) = 0.030474214560266333\n",
      "InfoGain(standard-of-living | class) = 0.03251146005380634\n",
      "InfoGain(media-exposure | class) = 0.015786455595619975\n",
      "Accuracy for Testing on the Training Data: 50.58%\n",
      "\n",
      "10-FOLD CROSS VALIDATION\n",
      "... ...\n",
      "Accuracy using k-Fold Cross Validation: 51.70% 44.90% 53.74% 48.98% 50.00% 48.98% 49.32% 53.06% 49.32% 42.86%\n",
      "Average 10-Fold Cross Validation Accuracy: 49.29%\n",
      "****************************************\n",
      "Processing hepatitis.csv ...\n",
      "TESTING ON THE TRAIN DATA\n",
      "InfoGain(sex | class) = 0.03660746514280977\n",
      "InfoGain(steroid | class) = 0.013721237647576712\n",
      "InfoGain(antivirals | class) = 0.014490701150154384\n",
      "InfoGain(fatigue | class) = 0.08315059666094515\n",
      "InfoGain(malaise | class) = 0.08228641352644739\n",
      "InfoGain(anorexia | class) = 0.011963525360643712\n",
      "InfoGain(liver-big | class) = 0.00702561259050305\n",
      "InfoGain(liver-firm | class) = 0.0002889437074383716\n",
      "InfoGain(spleen-palpable | class) = 0.03517926772302904\n",
      "InfoGain(spiders | class) = 0.10367840650872329\n",
      "InfoGain(ascites | class) = 0.1275186922033822\n",
      "InfoGain(varices | class) = 0.07645578607607972\n",
      "InfoGain(histology | class) = 0.08493296456638777\n",
      "Accuracy for Testing on the Training Data: 84.52%\n",
      "\n",
      "10-FOLD CROSS VALIDATION\n",
      "... ...\n",
      "Accuracy using k-Fold Cross Validation: 81.25% 86.67% 100.00% 80.00% 81.25% 93.33% 68.75% 86.67% 62.50% 86.67%\n",
      "Average 10-Fold Cross Validation Accuracy: 82.71%\n",
      "****************************************\n",
      "Processing hypothyroid.csv ...\n",
      "TESTING ON THE TRAIN DATA\n",
      "InfoGain(sex | class) = 0.0002272085595206863\n",
      "InfoGain(on-thyroxine | class) = 0.0009139351160850073\n",
      "InfoGain(query-on-thyroxine | class) = 0.0012382074503017315\n",
      "InfoGain(on_antithyroid | class) = 0.00014844815831743796\n",
      "InfoGain(surgery | class) = 0.0009985293906336068\n",
      "InfoGain(query-hypothyroid | class) = 0.0013683791752741592\n",
      "InfoGain(query-hyperthyroid | class) = 0.0005423006444423839\n",
      "InfoGain(pregnant | class) = 0.0004350938464638965\n",
      "InfoGain(sick | class) = 0.0004888757691284829\n",
      "InfoGain(tumor | class) = 0.0008983004044028076\n",
      "InfoGain(lithium | class) = 4.463778824304043e-05\n",
      "InfoGain(goitre | class) = 7.868469847943649e-05\n",
      "InfoGain(TSH | class) = 0.009353710215580346\n",
      "InfoGain(T3 | class) = 0.004075493419623766\n",
      "InfoGain(TT4 | class) = 0.005792553705846859\n",
      "InfoGain(T4U | class) = 0.005768288201614624\n",
      "InfoGain(FTI | class) = 0.005744031245602799\n",
      "InfoGain(TBG | class) = 0.002580427555574416\n",
      "Accuracy for Testing on the Training Data: 95.23%\n",
      "\n",
      "10-FOLD CROSS VALIDATION\n",
      "... ...\n",
      "Accuracy using k-Fold Cross Validation: 94.62% 96.20% 95.57% 94.62% 95.25% 94.32% 96.52% 93.38% 93.67% 98.11%\n",
      "Average 10-Fold Cross Validation Accuracy: 95.23%\n",
      "****************************************\n",
      "Processing mushroom.csv ...\n",
      "TESTING ON THE TRAIN DATA\n",
      "InfoGain(cap-shape | class) = 0.04879670193537311\n",
      "InfoGain(cap-surface | class) = 0.028590232773772706\n",
      "InfoGain(cap-color | class) = 0.03604928297620391\n",
      "InfoGain(bruises | class) = 0.19237948576121977\n",
      "InfoGain(odor | class) = 0.9060749773839999\n",
      "InfoGain(gill-attachment | class) = 0.014165027250616302\n",
      "InfoGain(gill-spacing | class) = 0.10088318399657048\n",
      "InfoGain(gill-size | class) = 0.23015437514804604\n",
      "InfoGain(gill-color | class) = 0.41697752341613137\n",
      "InfoGain(stalk-shape | class) = 0.007516772569664321\n",
      "InfoGain(stalk-root | class) = 0.10834857380797525\n",
      "InfoGain(stalk-surface-above-ring | class) = 0.2847255992184845\n",
      "InfoGain(stalk-surface-below-ring | class) = 0.2718944733927465\n",
      "InfoGain(stalk-color-above-ring | class) = 0.2538451734622399\n",
      "InfoGain(stalk-color-below-ring | class) = 0.24141556652756657\n",
      "InfoGain(veil-color | class) = 0.02381701612091669\n",
      "InfoGain(ring-number | class) = 0.03845266924309054\n",
      "InfoGain(ring-type | class) = 0.3180215107935377\n",
      "InfoGain(spore-print-color | class) = 0.4807049176849154\n",
      "InfoGain(population | class) = 0.2019580190668524\n",
      "InfoGain(habitat | class) = 0.156833604605092\n",
      "Accuracy for Testing on the Training Data: 99.58%\n",
      "\n",
      "10-FOLD CROSS VALIDATION\n",
      "... ...\n",
      "Accuracy using k-Fold Cross Validation: 99.51% 99.63% 100.00% 99.75% 99.75% 99.51% 99.01% 99.51% 99.51% 99.75%\n",
      "Average 10-Fold Cross Validation Accuracy: 99.59%\n",
      "****************************************\n",
      "Processing nursery.csv ...\n",
      "TESTING ON THE TRAIN DATA\n",
      "InfoGain(parents | class) = 0.07293460750309944\n",
      "InfoGain(has_nurs | class) = 0.1964492804881155\n",
      "InfoGain(form | class) = 0.005572591715219843\n",
      "InfoGain(children | class) = 0.011886431475775838\n",
      "InfoGain(housing | class) = 0.019602025022872116\n",
      "InfoGain(finance | class) = 0.0043331270252000564\n",
      "InfoGain(social | class) = 0.02223261689401812\n",
      "InfoGain(health | class) = 0.9587749604699762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Testing on the Training Data: 90.31%\n",
      "\n",
      "10-FOLD CROSS VALIDATION\n",
      "... ...\n",
      "Accuracy using k-Fold Cross Validation: 89.04% 89.35% 90.20% 91.20% 90.74% 91.13% 90.66% 91.20% 89.35% 90.59%\n",
      "Average 10-Fold Cross Validation Accuracy: 90.35%\n",
      "****************************************\n",
      "Processing primary-tumor.csv ...\n",
      "TESTING ON THE TRAIN DATA\n",
      "InfoGain(age | class) = 0.15474214188705826\n",
      "InfoGain(sex | class) = 0.32289865796149675\n",
      "InfoGain(histologic-type | class) = 0.3103754746615519\n",
      "InfoGain(degree-of-diffe | class) = 0.199205643547415\n",
      "InfoGain(bone | class) = 0.2124618990481646\n",
      "InfoGain(bone-marrow | class) = 0.020366938848046967\n",
      "InfoGain(lung | class) = 0.10088123982398978\n",
      "InfoGain(pleura | class) = 0.0678727757044224\n",
      "InfoGain(peritoneum | class) = 0.22052193470670467\n",
      "InfoGain(liver | class) = 0.19976143639025112\n",
      "InfoGain(brain | class) = 0.06714460241010434\n",
      "InfoGain(skin | class) = 0.054013531994907105\n",
      "InfoGain(neck | class) = 0.2915301360224922\n",
      "InfoGain(supraclavicular | class) = 0.12715354518198208\n",
      "InfoGain(axillar | class) = 0.24010574063173618\n",
      "InfoGain(mediastinum | class) = 0.18425767171538432\n",
      "InfoGain(abdominal | class) = 0.1701481108388716\n",
      "Accuracy for Testing on the Training Data: 57.52%\n",
      "\n",
      "10-FOLD CROSS VALIDATION\n",
      "... ...\n",
      "Accuracy using k-Fold Cross Validation: 50.00% 41.18% 47.06% 47.06% 55.88% 35.29% 58.82% 41.18% 44.12% 39.39%\n",
      "Average 10-Fold Cross Validation Accuracy: 46.00%\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Script to run everything\"\"\"\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "print(\"*\"*40)\n",
    "k = int(input(\"Enter k value for k-Fold Cross Validation: \"))\n",
    "\"\"\"DO WE WANT TO DROP 0 INFO GAIN ATTRIBUTES?\"\"\"\n",
    "drop = input(\"Drop all columns with absolutely no information gain? (y/n): \").lower()\n",
    "\"\"\"DO WE WANT TO PRINT THE INFO GAIN?\"\"\"\n",
    "to_print = input(\"Print the information gain? (y/n): \").lower()\n",
    "print(\"*\"*40)\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "for data in datasets:\n",
    "    print(f\"Processing {data} ...\")\n",
    "    print(\"TESTING ON THE TRAIN DATA\")\n",
    "    \n",
    "    df = preprocess(data)\n",
    "    if drop in 'yesYesYES':\n",
    "        df = info_gain(df, drop_no_gain = True)\n",
    "        info_gain_given_class = info_gain(df)\n",
    "    else:\n",
    "        info_gain_given_class = info_gain(df)\n",
    "    if to_print in 'yesYesYES':\n",
    "        for attribute in info_gain_given_class:\n",
    "            print(f'InfoGain({attribute} | class) = {info_gain_given_class[attribute]}')\n",
    "            \n",
    "    \"\"\"TRAIN / TEST\"\"\"\n",
    "    model = train(df)\n",
    "    prediction = predict(model, df)\n",
    "    results = evaluate(prediction, df)\n",
    "    print(f\"Accuracy for Testing on the Training Data: {100*sum(results)/len(results):.2f}%\")\n",
    "    \n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    \n",
    "    print(f\"\\n{k}-FOLD CROSS VALIDATION\")\n",
    "    \n",
    "    cross_validation_pairs = preprocess(data, testing_on_train = False, k = k, drop = drop)\n",
    "    trained_models = cross_validation_train(cross_validation_pairs)\n",
    "    print(\"...\",end=\" \")\n",
    "    predictions = cross_validation_predict(trained_models, cross_validation_pairs)\n",
    "    print(\"...\")\n",
    "    cross_validation_results = cross_validation_evaluate(predictions, cross_validation_pairs)\n",
    "    print(\"Accuracy using k-Fold Cross Validation: \" + \" \".join([f\"{100*sum(i) / len(i):.2f}%\" for i in cross_validation_results]))\n",
    "    print(f\"Average {k}-Fold Cross Validation Accuracy: {sum([100*sum(i) / len(i) for i in cross_validation_results]) / len(cross_validation_results):.2f}%\")\n",
    "    print(\"*\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions (you may respond in a cell or cells below):\n",
    "\n",
    "1. The Naive Bayes classifiers can be seen to vary, in terms of their effectiveness on the given datasets (e.g. in terms of Accuracy). Consider the Information Gain of each attribute, relative to the class distribution — does this help to explain the classifiers’ behaviour? Identify any results that are particularly surprising, and explain why they occur.\n",
    "2. The Information Gain can be seen as a kind of correlation coefficient between a pair of attributes: when the gain is low, the attribute values are uncorrelated; when the gain is high, the attribute values are correlated. In supervised ML, we typically calculate the Infomation Gain between a single attribute and the class, but it can be calculated for any pair of attributes. Using the pair-wise IG as a proxy for attribute interdependence, in which cases are our NB assumptions violated? Describe any evidence (or indeed, lack of evidence) that this is has some effect on the effectiveness of the NB classifier.\n",
    "3. Since we have gone to all of the effort of calculating Infomation Gain, we might as well use that as a criterion for building a “Decision Stump” (1-R classifier). How does the effectiveness of this classifier compare to Naive Bayes? Identify one or more cases where the effectiveness is notably different, and explain why.\n",
    "4. Evaluating the model on the same data that we use to train the model is considered to be a major mistake in Machine Learning. Implement a hold–out or cross–validation evaluation strategy. How does your estimate of effectiveness change, compared to testing on the training data? Explain why. (The result might surprise you!)\n",
    "5. Implement one of the advanced smoothing regimes (add-k, Good-Turing). Does changing the smoothing regime (or indeed, not smoothing at all) affect the effectiveness of the Naive Bayes classifier? Explain why, or why not.\n",
    "6. Naive Bayes is said to elegantly handle missing attribute values. For the datasets with missing values, is there any evidence that the performance is different on the instances with missing values, compared to the instances where all of the values are present? Does it matter which, or how many values are missing? Would a imputation strategy have any effect on this?\n",
    "\n",
    "Don't forget that groups of 1 student should respond to question (1), and one other question of your choosing. Groups of 2 students should respond to question (1) and question (2), and two other questions of your choosing. Your responses should be about 150-250 words each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Should improve since a using the training dataset to test a supervised algo is like a cheat. the supervised algo has already seen the answers and will recognise it. Using a test / cross-validation will ensure that we are left with unseen data albeit a decrease in training instances\n",
    "\n",
    "6. performance wise there should be no difference. there is no need to impute as a missing value could be significant in implying something. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
