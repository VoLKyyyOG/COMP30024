{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2019 Semester 1\n",
    "-----\n",
    "## Project 1: Gaining Information about Naive Bayes\n",
    "-----\n",
    "###### Student Name(s): Akira and Callum\n",
    "###### Python version: 3.7.1 from Anaconda \n",
    "###### Submission deadline: 1pm, Fri 5 Apr 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you may use for your Project 1 submission. (You are not required to use it; in particular, there is no need to use iPython if you do not like it.)\n",
    "\n",
    "Marking will be applied on the five functions that are defined in this notebook, and to your responses to the questions at the end of this notebook.\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The cell below supresses forced output scrolling so you can see view the script output easier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We have specifically imported all functions below to ensure we implemented iteratively AND without the use of external functions wherever possible**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv, DataFrame, Series\n",
    "from collections import defaultdict, Counter\n",
    "from numpy import NaN\n",
    "from math import log\n",
    "\n",
    "########## POSSIBLE CSVs ##########\n",
    "d1 =  'anneal.csv'\n",
    "h1 = 'family,product-type,steel,carbon,hardness,temper_rolling,condition,formability,strength,non-ageing,surface-finish,surface-quality,enamelability,bc,bf,bt,bw-me,bl,m,chrom,phos,cbond,marvi,exptl,ferro,corr,bbvc,lustre,jurofm,s,p,shape,oil,bore,packing,class'.split(',')\n",
    "\n",
    "d2 =  'breast-cancer.csv'\n",
    "h2 = 'age,menopause,tumor-size,inv-nodes,node-caps,deg-malig,breast,breast-quad,irradiat,class'.split(',')\n",
    "\n",
    "d3 =  'car.csv'\n",
    "h3 = 'buying,maint,doors,persons,lug_boot,safety,class'.split(',')\n",
    "\n",
    "d4 =  'cmc.csv'\n",
    "h4 = 'w-education,h-education,n-child,w-relation,w-work,h-occupation,standard-of-living,media-exposure,class'.split(',')\n",
    "\n",
    "d5 =  'hepatitis.csv'\n",
    "h5 = 'sex,steroid,antivirals,fatigue,malaise,anorexia,liver-big,liver-firm,spleen-palpable,spiders,ascites,varices,histology,class'.split(',')\n",
    "\n",
    "d6 =  'hypothyroid.csv'\n",
    "h6 = 'sex,on-thyroxine,query-on-thyroxine,on_antithyroid,surgery,query-hypothyroid,query-hyperthyroid,pregnant,sick,tumor,lithium,goitre,TSH,T3,TT4,T4U,FTI,TBG,class'.split(',')\n",
    "\n",
    "d7 =  'mushroom.csv'\n",
    "h7 = 'cap-shape,cap-surface,cap-color,bruises,odor,gill-attachment,gill-spacing,gill-size,gill-color,stalk-shape,stalk-root,stalk-surface-above-ring,stalk-surface-below-ring,stalk-color-above-ring,stalk-color-below-ring,veil-type,veil-color,ring-number,ring-type,spore-print-color,population,habitat,class'.split(',')\n",
    "\n",
    "d8 =  'nursery.csv'\n",
    "h8 = 'parents,has_nurs,form,children,housing,finance,social,health,class'.split(',')\n",
    "\n",
    "d9 = 'primary-tumor.csv'\n",
    "h9 = 'age,sex,histologic-type,degree-of-diffe,bone,bone-marrow,lung,pleura,peritoneum,liver,brain,skin,neck,supraclavicular,axillar,mediastinum,abdominal,class'.split(',')\n",
    "\n",
    "datasets = [d1,d2,d3,d4,d5,d6,d7,d8,d9]\n",
    "dataset_headers = [h1,h2,h3,h4,h5,h6,h7,h8,h9]\n",
    "\n",
    "dictionary = {datasets[i] : dataset_headers[i] for i in range(len(datasets))}\n",
    "\n",
    "\"\"\"Sets the column of each dataset\"\"\"\n",
    "def set_column(filename):\n",
    "    return dictionary[filename]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Has been adjusted so that it works with testing on the train data, and partitioning for cross_val\"\"\"\n",
    "def preprocess(filename, testing_on_train = True, k = 10, drop = 'no', impute = 'no'):\n",
    "    # Add column headers, drop columns with only one unique value (all same value)\n",
    "    df = read_csv(filename, header = None, names = set_column(filename))\n",
    "    df.replace('?', NaN, inplace=True)\n",
    "    \n",
    "    if impute in 'yesYesYES':    \n",
    "        mode = df.mode().iloc[0]\n",
    "        df = df.fillna(mode)\n",
    "    \n",
    "    \"\"\"If we are not using the cross validation method\"\"\"\n",
    "    # Return the whole dataset as a dataframe\n",
    "    if testing_on_train:\n",
    "        return df\n",
    "    else:\n",
    "        \"\"\"Drop no gain attributes given the input in the script\"\"\"\n",
    "        if drop in 'yesYesYES':\n",
    "            non_unique = df.apply(Series.nunique)\n",
    "            df.drop(non_unique[non_unique == 1].index, axis=1, inplace=True)\n",
    "            \n",
    "        temp = df.copy()\n",
    "        partitions = list()\n",
    "        \n",
    "        # k-fold Cross Validation\n",
    "        divisor = k\n",
    "        \n",
    "        for i in range(k):\n",
    "            partitions.append(temp.sample(frac=1/divisor))\n",
    "            divisor -= 1\n",
    "            temp.drop(partitions[-1].index, axis=0, inplace=True)\n",
    "        \n",
    "        del temp\n",
    "        \n",
    "        # Dictionary of train/test pairs\n",
    "        cross_validation_pairs = defaultdict(list)\n",
    "        models = list()\n",
    "        \n",
    "        for i in range(k):\n",
    "            test = partitions[i]\n",
    "            train = df.iloc[df.index.drop(test.index.values)]\n",
    "            cross_validation_pairs[\"train\"].append(train)\n",
    "            cross_validation_pairs[\"test\"].append(test)\n",
    "            \n",
    "        return cross_validation_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Trains a model given a training dataset\"\"\"\n",
    "def train(train_set):\n",
    "    N = len(train_set)\n",
    "    priors = {}\n",
    "    posteriors = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))\n",
    "    \"\"\"Accessable using posteriors[class j][attribute x][value i]\"\"\"\n",
    "    \n",
    "    for label in train_set['class'].unique():\n",
    "        priors[label] = len(train_set.loc[train_set['class'] == label]) / N\n",
    "        for attribute in train_set.columns[:-1]:\n",
    "            temp = train_set.loc[train_set['class'] == label, [attribute,'class']]\n",
    "            n = len(temp)\n",
    "            \n",
    "            count = defaultdict(int)\n",
    "            for i in temp[attribute]:\n",
    "                if i != NaN:\n",
    "                    count[i] += 1\n",
    "            \n",
    "            for i in count:\n",
    "                posteriors[label][attribute][i] = count[i] / n\n",
    "                \n",
    "    trained_model = {\"priors\": priors, \"posteriors\": posteriors}\n",
    "    \n",
    "    return trained_model\n",
    "\n",
    "\"\"\"Trains M partitions for cross_val\"\"\"\n",
    "def cross_validation_train(cross_validation_pairs):\n",
    "    trained_models = list()\n",
    "    train_set = cross_validation_pairs[\"train\"]\n",
    "    test_set = cross_validation_pairs[\"test\"]\n",
    "    N = len(train_set)\n",
    "    \n",
    "    for i in range(N):\n",
    "        trained_models.append(train(train_set[i]))\n",
    "        \n",
    "    return trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Predicts a test set\"\"\"\n",
    "def predict(trained_model, test_set):\n",
    "    priors = trained_model[\"priors\"]\n",
    "    posteriors = trained_model[\"posteriors\"]\n",
    "    \n",
    "    \"\"\"Drop the class labels of the test set\"\"\"\n",
    "    test_labels = test_set['class']\n",
    "    test = test_set.drop('class', axis=1)\n",
    "    cols = test_set.columns\n",
    "    \n",
    "    \"\"\"Probabilistic Smoothing with epsilon -> 0\"\"\"\n",
    "    n = len(test_labels)\n",
    "    epsilon = 1e-100\n",
    "    \n",
    "    \"\"\"Model Prediction\"\"\"\n",
    "    prediction = {}\n",
    "    \n",
    "    \"\"\"The Predicted Labels to be Returned\"\"\"\n",
    "    \"\"\"(Key, Value) = (Test Instance Row, Predicted Label)\"\"\"\n",
    "    predicted_labels = {}\n",
    "    \n",
    "    for i in range(n):\n",
    "        instance = test.iloc[i]\n",
    "        for label in priors.keys():       \n",
    "            prob = log(priors[label])/log(2)\n",
    "            for attribute in cols:\n",
    "                try:\n",
    "                    \"\"\"If the value is non missing\"\"\"\n",
    "                    if instance[attribute] != NaN:\n",
    "                        prob += log(posteriors[label][attribute][instance[attribute]])/log(2)\n",
    "                    else:\n",
    "                        \"\"\"Otherwise we have chosen to simply ignore it\"\"\"\n",
    "                        pass\n",
    "                except:\n",
    "                    \"\"\"If the value does not exist in our model, we use epsilon\"\"\"\n",
    "                    prob += log(epsilon)/log(2)\n",
    "                    \n",
    "            prediction[label] = prob\n",
    "        \n",
    "        \"\"\"Choose the predicted class with the highest probability\"\"\"\n",
    "        predicted_labels[i] = max(prediction, key=prediction.get)\n",
    "        \n",
    "    return predicted_labels\n",
    "\n",
    "\"\"\"Tests partitions\"\"\"\n",
    "def cross_validation_predict(trained_models, cross_validation_pairs):\n",
    "    test_set = cross_validation_pairs[\"test\"]\n",
    "    N = len(test_set)\n",
    "    predictions = list()\n",
    "    \n",
    "    for i in range(N):\n",
    "        predictions.append(predict(trained_models[i], test_set[i]))\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluates the accuracy\"\"\"\n",
    "def evaluate(predicted_labels, test_set):\n",
    "    test_labels = test_set['class']\n",
    "    n = len(test_labels)\n",
    "    \n",
    "    return [1 if predicted_labels[i] == test_labels.iloc[i] else 0 for i in range(n)]\n",
    "\n",
    "\"\"\"Evaluates cross_val accuracy\"\"\"\n",
    "def cross_validation_evaluate(predictions, cross_pairs):\n",
    "    test = cross_pairs[\"test\"]\n",
    "    N = len(test)\n",
    "    results = list()\n",
    "    for i in range(N):\n",
    "        results.append(evaluate(predictions[i], test[i]))\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Calculates the entropy given a series\"\"\"\n",
    "def entropy(distribution):\n",
    "    # Calculates the entropy (uncertainty) of an event (class) for a given distribution \n",
    "    # (e.g. distribution given a certain attribute value)\n",
    "    N = len(distribution)\n",
    "    # Normalise and count values iteratively\n",
    "    event = [i/N for i in Counter(distribution).values()]\n",
    "    \n",
    "    # Will lose marks for this since it is an aggregate function according to forums\n",
    "    # The iterative approach is off by 0.000000000000000xx compared to the .value_counts() method\n",
    "    # event = pd.Series(attribute_value).value_counts(normalize=True, sort=False)\n",
    "    \n",
    "    return (-1*sum([i*log(i)/log(2) for i in event]))\n",
    "\n",
    "\"\"\"Calculates the mean information given a dataset\"\"\"\n",
    "def mean_info(dataset):\n",
    "    mean_info_per_attribute = defaultdict(float)\n",
    "    \n",
    "    for attribute in dataset.columns[:-1]:\n",
    "        # Used to calculate 'probability' of said value for a random instance (number of values / total number of instances)\n",
    "        attribute_value_counts = Counter(dataset[attribute])\n",
    "        N = len(dataset[attribute])\n",
    "        for value in dataset[attribute].unique():\n",
    "            # dataframe.loc on said value and return the corresponding class column\n",
    "            corresponding_values = dataset.loc[dataset[attribute] == value, 'class']\n",
    "            # add (probability of said value * entropy of said value) to the attribute's mean info\n",
    "            mean_info_per_attribute[attribute] += attribute_value_counts[value]/N * entropy(corresponding_values)\n",
    "            \n",
    "    return mean_info_per_attribute\n",
    "\n",
    "\"\"\"Calculates the information gain given a dataset. Adjusted so that it can drop 0 info_gain columns\"\"\"\n",
    "def info_gain(dataset, drop_no_gain = False):\n",
    "    mean_info_per_attribute = mean_info(dataset) # Weighted sum of child stump entropies if split on each attribute\n",
    "    class_entropy = entropy(dataset['class']) # H(R), the entropy of class distribution prior to splitting\n",
    "    info_gain_given_class = defaultdict(float)\n",
    "    \n",
    "    for attribute in mean_info_per_attribute:\n",
    "        # Calculates IG(attribute) for the dataset's class\n",
    "        info_gain_given_class[attribute] = class_entropy - mean_info_per_attribute[attribute]\n",
    "    \n",
    "    \"\"\"If we want to drop the columns with absolutely 0 information gain\"\"\"\n",
    "    if drop_no_gain:\n",
    "        non_unique = dataset.apply(Series.nunique)\n",
    "        dataset.drop(non_unique[non_unique == 1].index, axis=1, inplace=True)\n",
    "        return dataset\n",
    "    else:\n",
    "        return info_gain_given_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_attributes(distribution):\n",
    "    N = len(distribution)\n",
    "    event = [i/N for i in Counter(distribution).values()]\n",
    "    return (-1*sum([i*log(i)/log(2) for i in event]))\n",
    "\n",
    "def mean_info_attributes(dataset, column_name):\n",
    "    mean_info_per_attribute = defaultdict(float)\n",
    "    \n",
    "    # We want every OTHER attribute \n",
    "    cols = [i for i in dataset.columns if i != column_name]\n",
    "    \n",
    "    for attribute in cols: # class label has been dropped already\n",
    "        attribute_value_counts = Counter(dataset[attribute])\n",
    "        N = len(dataset[attribute])\n",
    "        for value in dataset[attribute].unique():\n",
    "            corresponding_values = dataset.loc[dataset[attribute] == value, attribute]\n",
    "            mean_info_per_attribute[attribute] += attribute_value_counts[value]/N * entropy_attributes(corresponding_values)\n",
    "            \n",
    "    return mean_info_per_attribute\n",
    "\n",
    "def info_gain_attributes(dataset, to_print = 'no'):\n",
    "    temp = dataset.drop('class', axis=1)\n",
    "    \n",
    "    entropy_per_attribute = defaultdict(float)\n",
    "    all_IG = defaultdict(float) # Me change\n",
    "    \n",
    "    for attribute in temp.columns:\n",
    "        # a dictionary of H(R)\n",
    "        entropy_per_attribute[attribute] = entropy(temp[attribute])\n",
    "        \n",
    "        mean_info_per_attribute = mean_info_attributes(temp, attribute)\n",
    "        \n",
    "    for attribute in entropy_per_attribute:\n",
    "        if to_print in 'yesYesYES': # Me change\n",
    "            print(f\"Entropy({attribute}) = {entropy_per_attribute[attribute]:.4f}\\n\")\n",
    "        average = list()\n",
    "        for every_other_attribute in mean_info_per_attribute:\n",
    "            info = entropy_per_attribute[attribute] - mean_info_per_attribute[every_other_attribute]\n",
    "            average.append(info)\n",
    "            if to_print in 'yesYesYES':\n",
    "                print(f\"InfoGain({every_other_attribute} | {attribute}) = {info:.4f}\")\n",
    "        \n",
    "        if to_print in 'yesYesYES': # Me change\n",
    "            print(f\"\\nAverage Info Gain for {attribute} to every other attribute is {sum(average)/len(mean_info_per_attribute):.4f}\")\n",
    "            print(\"*\"*40)\n",
    "        \n",
    "        all_IG[attribute]= sum(average)/len(mean_info_per_attribute) # Me change\n",
    "    return all_IG # Me change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here is a script that runs every dataset and tests on its training, as well as k-fold cross-validation for a given _k_. It will also ask (y/n) for printing relevant information gain, and to drop attributes with 0 information gain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "Enter k value for k-Fold Cross Validation: 10\n",
      "Drop all columns with absolutely no information gain? (y/n): y\n",
      "Print the information gain? (y/n): n\n",
      "Impute missing values? (y/n): n\n",
      "****************************************\n",
      "Processing anneal.csv ...\n",
      "For anneal.csv\n",
      "Number of rows: 898\n",
      "Number of attributes/columns: 29\n",
      "avg 0.6146, best 2.0508, worst 0.0125, var 0.3208\n",
      "TESTING ON THE TRAIN DATA\n",
      "Accuracy for Testing on the Training Data: 99.11%\n",
      "\n",
      "10-FOLD CROSS VALIDATION\n",
      "...\n",
      "Accuracy using k-Fold Cross Validation: 100.00% 98.89% 95.56% 98.89% 98.89% 97.78% 100.00% 100.00% 100.00% 100.00%\n",
      "Average 10-Fold Cross Validation Accuracy: 99.00%\n",
      "****************************************\n",
      "Processing breast-cancer.csv ...\n",
      "For breast-cancer.csv\n",
      "Number of rows: 286\n",
      "Number of attributes/columns: 10\n",
      "avg 1.5285, best 3.0244, worst 0.7913, var 0.4640\n",
      "TESTING ON THE TRAIN DATA\n",
      "Accuracy for Testing on the Training Data: 75.52%\n",
      "\n",
      "10-FOLD CROSS VALIDATION\n",
      "...\n",
      "Accuracy using k-Fold Cross Validation: 82.76% 75.86% 71.43% 82.76% 60.71% 72.41% 78.57% 72.41% 64.29% 68.97%\n",
      "Average 10-Fold Cross Validation Accuracy: 73.02%\n",
      "****************************************\n",
      "Processing car.csv ...\n",
      "For car.csv\n",
      "Number of rows: 1728\n",
      "Number of attributes/columns: 7\n",
      "avg 1.7925, best 2.0000, worst 1.5850, var 0.0431\n",
      "TESTING ON THE TRAIN DATA\n",
      "Accuracy for Testing on the Training Data: 87.38%\n",
      "\n",
      "10-FOLD CROSS VALIDATION\n",
      "...\n",
      "Accuracy using k-Fold Cross Validation: 85.55% 87.86% 87.28% 86.71% 92.49% 87.28% 83.72% 83.24% 82.56% 85.55%\n",
      "Average 10-Fold Cross Validation Accuracy: 86.22%\n",
      "****************************************\n",
      "Processing cmc.csv ...\n",
      "For cmc.csv\n",
      "Number of rows: 1473\n",
      "Number of attributes/columns: 9\n",
      "avg 1.3798, best 2.4929, worst 0.3807, var 0.4531\n",
      "TESTING ON THE TRAIN DATA\n",
      "Accuracy for Testing on the Training Data: 50.58%\n",
      "\n",
      "10-FOLD CROSS VALIDATION\n",
      "...\n",
      "Accuracy using k-Fold Cross Validation: 47.62% 51.02% 42.86% 46.94% 55.41% 53.06% 47.97% 55.10% 50.00% 46.26%\n",
      "Average 10-Fold Cross Validation Accuracy: 49.62%\n",
      "****************************************\n",
      "Processing hepatitis.csv ...\n",
      "For hepatitis.csv\n",
      "Number of rows: 155\n",
      "Number of attributes/columns: 14\n",
      "avg 0.8968, best 1.2799, worst 0.4791, var 0.0433\n",
      "TESTING ON THE TRAIN DATA\n",
      "Accuracy for Testing on the Training Data: 84.52%\n",
      "\n",
      "10-FOLD CROSS VALIDATION\n",
      "...\n",
      "Accuracy using k-Fold Cross Validation: 87.50% 60.00% 68.75% 60.00% 81.25% 93.33% 81.25% 86.67% 93.75% 100.00%\n",
      "Average 10-Fold Cross Validation Accuracy: 81.25%\n",
      "****************************************\n",
      "Processing hypothyroid.csv ...\n",
      "For hypothyroid.csv\n",
      "Number of rows: 3163\n",
      "Number of attributes/columns: 19\n",
      "avg 0.3578, best 1.0119, worst 0.0076, var 0.0636\n",
      "TESTING ON THE TRAIN DATA\n",
      "Accuracy for Testing on the Training Data: 95.23%\n",
      "\n",
      "10-FOLD CROSS VALIDATION\n",
      "...\n",
      "Accuracy using k-Fold Cross Validation: 95.57% 95.57% 94.30% 96.20% 95.25% 94.95% 96.52% 94.64% 93.99% 95.27%\n",
      "Average 10-Fold Cross Validation Accuracy: 95.23%\n",
      "****************************************\n",
      "Processing mushroom.csv ...\n",
      "For mushroom.csv\n",
      "Number of rows: 8124\n",
      "Number of attributes/columns: 22\n",
      "avg 1.5119, best 3.0304, worst 0.1731, var 0.5854\n",
      "TESTING ON THE TRAIN DATA\n",
      "Accuracy for Testing on the Training Data: 99.72%\n",
      "\n",
      "10-FOLD CROSS VALIDATION\n",
      "...\n",
      "Accuracy using k-Fold Cross Validation: 99.88% 99.14% 100.00% 99.88% 100.00% 99.63% 99.26% 99.75% 99.63% 99.88%\n",
      "Average 10-Fold Cross Validation Accuracy: 99.70%\n",
      "****************************************\n",
      "Processing nursery.csv ...\n",
      "For nursery.csv\n",
      "Number of rows: 12960\n",
      "Number of attributes/columns: 9\n",
      "avg 1.7077, best 2.3219, worst 1.0000, var 0.1387\n",
      "TESTING ON THE TRAIN DATA\n",
      "Accuracy for Testing on the Training Data: 90.31%\n",
      "\n",
      "10-FOLD CROSS VALIDATION\n",
      "...\n",
      "Accuracy using k-Fold Cross Validation: 91.67% 90.90% 91.05% 90.05% 90.90% 91.20% 89.97% 89.12% 87.89% 90.28%\n",
      "Average 10-Fold Cross Validation Accuracy: 90.30%\n",
      "****************************************\n",
      "Processing primary-tumor.csv ...\n",
      "For primary-tumor.csv\n",
      "Number of rows: 339\n",
      "Number of attributes/columns: 18\n",
      "avg 0.8143, best 1.7590, worst 0.1451, var 0.1487\n",
      "TESTING ON THE TRAIN DATA\n",
      "Accuracy for Testing on the Training Data: 61.65%\n",
      "\n",
      "10-FOLD CROSS VALIDATION\n",
      "...\n",
      "Accuracy using k-Fold Cross Validation: 44.12% 55.88% 44.12% 50.00% 38.24% 44.12% 47.06% 47.06% 29.41% 36.36%\n",
      "Average 10-Fold Cross Validation Accuracy: 43.64%\n",
      "****************************************\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.124663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.124663</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0  1.000000 -0.124663\n",
       "1 -0.124663  1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "print(\"*\"*40)\n",
    "k = int(input(\"Enter k value for k-Fold Cross Validation: \"))\n",
    "\"\"\"DO WE WANT TO DROP 0 INFO GAIN ATTRIBUTES?\"\"\"\n",
    "drop = input(\"Drop all columns with absolutely no information gain? (y/n): \").lower()\n",
    "\"\"\"DO WE WANT TO PRINT THE INFO GAIN?\"\"\"\n",
    "to_print = input(\"Print the information gain? (y/n): \").lower()\n",
    "\"\"\"DO WE WANT TO IMPUTE MISSING VALUES FOR THE TRAINING SET\"\"\"\n",
    "to_impute = input(\"Impute missing values? (y/n): \").lower()\n",
    "print(\"*\"*40)\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "from numpy import var, array\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ig = defaultdict(float)\n",
    "acc = defaultdict(float)\n",
    "\n",
    "for data in datasets:\n",
    "    print(f\"Processing {data} ...\")\n",
    "    \n",
    "    df = preprocess(data, impute = to_impute)\n",
    "    if drop in 'yesYesYES':\n",
    "        df = info_gain(df, drop_no_gain = True)\n",
    "        info_gain_given_class = info_gain(df)\n",
    "    else:\n",
    "        info_gain_given_class = info_gain(df)\n",
    "    if to_print in 'yesYesYES':\n",
    "        for attribute in info_gain_given_class:\n",
    "            print(f'InfoGain({attribute} | class) = {info_gain_given_class[attribute]:.4f}')\n",
    "        print('...')\n",
    "\n",
    "    IG_per_attr = info_gain_attributes(df, to_print = to_print)\n",
    "    print(f\"For {data}\") # Me change\n",
    "    print(\"Number of rows: %d\" % len(df))\n",
    "    print(\"Number of attributes/columns: %d\" % len(df.columns))\n",
    "    print(f\"avg {sum(IG_per_attr.values())/len(IG_per_attr):.4f}, best {max(IG_per_attr.values()):.4f}, worst {min(IG_per_attr.values()):.4f}, var {var(array(list(IG_per_attr.values()))):.4f}\") # Me change\n",
    "\n",
    "    ig[data] = sum(IG_per_attr.values())/len(IG_per_attr)\n",
    "    \n",
    "    print(\"TESTING ON THE TRAIN DATA\")\n",
    "    \n",
    "\n",
    "\n",
    "    \"\"\"TRAIN / TEST\"\"\"\n",
    "    model = train(df)\n",
    "    prediction = predict(model, df)\n",
    "    results = evaluate(prediction, df)\n",
    "    print(f\"Accuracy for Testing on the Training Data: {100*sum(results)/len(results):.2f}%\")\n",
    "    \n",
    "    acc[data] = 100*sum(results)/len(results)\n",
    "\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "    print(f\"\\n{k}-FOLD CROSS VALIDATION\")\n",
    "\n",
    "    cross_validation_pairs = preprocess(data, testing_on_train = False, k = k, drop = drop)\n",
    "    trained_models = cross_validation_train(cross_validation_pairs)\n",
    "    print(\"...\")\n",
    "    predictions = cross_validation_predict(trained_models, cross_validation_pairs)\n",
    "    cross_validation_results = cross_validation_evaluate(predictions, cross_validation_pairs)\n",
    "    print(\"Accuracy using k-Fold Cross Validation: \" + \" \".join([f\"{100*sum(i) / len(i):.2f}%\" for i in cross_validation_results]))\n",
    "    print(f\"Average {k}-Fold Cross Validation Accuracy: {sum([100*sum(i) / len(i) for i in cross_validation_results]) / len(cross_validation_results):.2f}%\")\n",
    "    print(\"*\"*40)\n",
    "    \n",
    "    \n",
    "# Question 1 and 2 PROOF\n",
    "mapper = defaultdict(list)\n",
    "for i in ig.keys():\n",
    "    mapper[i].append(ig[i])\n",
    "    mapper[i].append(acc[i])\n",
    "mapper.values()\n",
    "plt.scatter([i[0] for i in mapper.values()], [i[1] for i in mapper.values()])\n",
    "plt.title(\"Naive Bayes Assumption Proof\")\n",
    "plt.xlabel(\"Average IG between Attribute Pairs\")\n",
    "plt.ylabel(\"Naive Bayes Accuracy\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "display(DataFrame.from_dict(mapper).T.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Questions 1, 2, 4 and 6 (150 - 200 words for each response):\n",
    "\n",
    "#### 1. The Naive Bayes classifiers can be seen to vary, in terms of their effectiveness on the given datasets (e.g. in terms of Accuracy). Consider the Information Gain of each attribute, relative to the class distribution — does this help to explain the classifiers’ behaviour? Identify any results that are particularly surprising, and explain why they occur.\n",
    "    - According to the Information Gain for each attribute given a class, we find that if there is ONE attribute with SIGNIFICANTLY MORE Information Gain COMPARED to every other attribute, we result in an overall HIGHER ACCURACY\n",
    "    - Datasets that have attributes with SIMILAR Information Gain with the class label will not perform as well as other datasets\n",
    "        - Can be seen with the `primary-tumor.csv` where all the information gain is roughly between 0.1 - 0.2\n",
    "        - Compare that to `mushroom.csv` where there is one attribute with 0.9 information gain, and the rest between 0.1 - 0.4 and some less than 0.1\n",
    "    - There are exceptions however - note 'hypothyroid.csv' whose attributes are very little information gain despite high Naive Bayes classification errors.y i dunno \n",
    "    \n",
    "\n",
    "#### 2. The Information Gain can be seen as a kind of correlation coefficient between a pair of attributes: when the gain is low, the attribute values are uncorrelated; when the gain is high, the attribute values are correlated. In supervised ML, we typically calculate the Infomation Gain between a single attribute and the class, but it can be calculated for any pair of attributes. Using the pair-wise IG as a proxy for attribute interdependence, in which cases are our NB assumptions violated? Describe any evidence (or indeed, lack of evidence) that this is has some effect on the effectiveness of the NB classifier.\n",
    "    - Our findings suggest that the Information Gain between attribute pairs are consistent\n",
    "    - Althought there are some attributes with significantly HIGH gain, it remains consistent throughout the dataset\n",
    "    - The Naive Bayes model assumes independence between all attributes, meaning that the Information Gain should be 0 throughout attribute pairs. In our pair-wise Information Gain calculations, we find that this is not the case and thus the assumption of Probabalistic Independence is violated. \n",
    "\n",
    "    - TAKE A READ THROUGH THIS https://stats.stackexchange.com/questions/23490/why-do-naive-bayesian-classifiers-perform-so-well\n",
    "    \n",
    "\n",
    "#### 4. Evaluating the model on the same data that we use to train the model is considered to be a major mistake in Machine Learning. Implement a hold–out or cross–validation evaluation strategy. How does your estimate of effectiveness change, compared to testing on the training data? Explain why. (The result might surprise you!)\n",
    "    - Implemented a k-fold Cross-Validation strategy\n",
    "    - Estimate of effectiveness does not change significantly (typically a 2% difference in accuracy where the results of k = 10 is in the table below)\n",
    "    - At most, it will perform the same as testing on the training data. If we take into account that testing on the training data is the \"best case accuracy\" for correctly predicted labels, then cross-validation does a reasonable job at maintaing a high accuracy, despite being partitioned. \n",
    "    - Although each instance will become a training instance and testing instance at one stage, there are better partitions to train on compared to others (limited to the fact that we are taking random samples of the dataset each time)\n",
    "    \n",
    "\n",
    "#### 6. Naive Bayes is said to elegantly handle missing attribute values. For the datasets with missing values, is there any evidence that the performance is different on the instances with missing values, compared to the instances where all of the values are present? Does it matter which, or how many values are missing? Would an imputation strategy have any effect on this?\n",
    "    - To an extent, it does matter how many values are missing, but it is also dependent if the attribute has a high information gain or not\n",
    "    - Missing values in itself could be siginficiant. i.e. Since there is a missing value, it could lead to a specific label class\n",
    "    - See table below for imputation impact on testing on the training data and 10-fold cross validation\n",
    "        - Can see that the training has no effect whether we impute values or not (makes sense since the model is supervised and has therefore seen the solution before)\n",
    "        - Imputing the missing values and then testing on imputed values is the same as skipping the missing values in training, and then skipping missing values in testing\n",
    "        - However, we can see that the *overall accuracy increases slightly* when testing using the 10-fold cross-validation method\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 6 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "display(read_csv('results.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
