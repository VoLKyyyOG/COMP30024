{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2019 Semester 1\n",
    "-----\n",
    "## Project 1: Gaining Information about Naive Bayes\n",
    "-----\n",
    "###### Student Name(s): Akira and Callum\n",
    "###### Python version: 3.7.1 from Anaconda \n",
    "###### Submission deadline: 1pm, Fri 5 Apr 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you may use for your Project 1 submission. (You are not required to use it; in particular, there is no need to use iPython if you do not like it.)\n",
    "\n",
    "Marking will be applied on the five functions that are defined in this notebook, and to your responses to the questions at the end of this notebook.\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrAiN sEt Is ThE tEsT sEt  \n",
    "Imputed train set with mode, and for test / crossval tests we ignore missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from collections import *\n",
    "import numpy as np\n",
    "from math import log\n",
    "\n",
    "########## POSSIBLE CSVs ##########\n",
    "d1 =  'anneal.csv'\n",
    "h1 = 'family,product-type,steel,carbon,hardness,temper_rolling,condition,formability,strength,non-ageing,surface-finish,surface-quality,enamelability,bc,bf,bt,bw-me,bl,m,chrom,phos,cbond,marvi,exptl,ferro,corr,bbvc,lustre,jurofm,s,p,shape,oil,bore,packing,class'.split(',')\n",
    "\n",
    "d2 =  'breast-cancer.csv'\n",
    "h2 = 'age,menopause,tumor-size,inv-nodes,node-caps,deg-malig,breast,breast-quad,irradiat,class'.split(',')\n",
    "\n",
    "d3 =  'car.csv'\n",
    "h3 = 'buying,maint,doors,persons,lug_boot,safety,class'.split(',')\n",
    "\n",
    "d4 =  'cmc.csv'\n",
    "h4 = 'w-education,h-education,n-child,w-relation,w-work,h-occupation,standard-of-living,media-exposure,class'.split(',')\n",
    "\n",
    "d5 =  'hepatitis.csv'\n",
    "h5 = 'sex,steroid,antivirals,fatigue,malaise,anorexia,liver-big,liver-firm,spleen-palpable,spiders,ascites,varices,histology,class'.split(',')\n",
    "\n",
    "d6 =  'hypothyroid.csv'\n",
    "h6 = 'sex,on-thyroxine,query-on-thyroxine,on_antithyroid,surgery,query-hypothyroid,query-hyperthyroid,pregnant,sick,tumor,lithium,goitre,TSH,T3,TT4,T4U,FTI,TBG,class'.split(',')\n",
    "\n",
    "d7 =  'mushroom.csv'\n",
    "h7 = 'cap-shape,cap-surface,cap-color,bruises,odor,gill-attachment,gill-spacing,gill-size,gill-color,stalk-shape,stalk-root,stalk-surface-above-ring,stalk-surface-below-ring,stalk-color-above-ring,stalk-color-below-ring,veil-type,veil-color,ring-number,ring-type,spore-print-color,population,habitat,class'.split(',')\n",
    "\n",
    "d8 =  'nursery.csv'\n",
    "h8 = 'parents,has_nurs,form,children,housing,finance,social,health,class'.split(',')\n",
    "\n",
    "d9 = 'primary-tumor.csv'\n",
    "h9 = 'age,sex,histologic-type,degree-of-diffe,bone,bone-marrow,lung,pleura,peritoneum,liver,brain,skin,neck,supraclavicular,axillar,mediastinum,abdominal,class'.split(',')\n",
    "\n",
    "datasets = [d1,d2,d3,d4,d5,d6,d7,d8,d9]\n",
    "dataset_headers = [h1,h2,h3,h4,h5,h6,h7,h8,h9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary that holds key 'filename' and values 'attribute names'\n",
    "dictionary = {}\n",
    "for i in range(9):\n",
    "    dictionary[datasets[i]] = dataset_headers[i]\n",
    "    \n",
    "########## FUNCTION DEFINITIONS ##########\n",
    "# Adds columns to the dataset\n",
    "def set_column(filename):\n",
    "    return dictionary[filename]\n",
    "\n",
    "# Used for print debugging - prints all the probabilities out\n",
    "def printer(posteriors, prob):\n",
    "    print(posteriors)\n",
    "    #########################################################################\n",
    "    # Let say I wanted Pr(Sex = M | hypothyroid) and Pr(Sex = M | negative)\n",
    "    # Access using dict[class label][attribute][attribute value] \n",
    "    \n",
    "    for class_label in posteriors:\n",
    "        print(f\"\\nPr({class_label}) = {prob}\")\n",
    "        for attribute in posteriors[class_label]:\n",
    "            for val in posteriors[class_label][attribute]:\n",
    "                print(f\"Pr({attribute} = {val} | class = {class_label}) = {posteriors[class_label][attribute][val]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should open a data file in csv, and transform it into a usable format \n",
    "def preprocess(filename):\n",
    "    # Read in csv and add the column header\n",
    "    df = pd.read_csv(filename, header = None, names = set_column(filename))\n",
    "    \n",
    "    # Drop columns that only have 1 unique value for every instance \n",
    "    non_unique = df.apply(pd.Series.nunique)\n",
    "    cols_to_drop = non_unique[non_unique == 1].index\n",
    "    df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    split = df.sample(frac=0.3)\n",
    "    train = df.drop(split.index)\n",
    "    \n",
    "    test = split.sample(frac=0.5)\n",
    "    split.drop(test.index, inplace=True)\n",
    "    crossval = split.copy()\n",
    "    \n",
    "    del df, split\n",
    "    \n",
    "    # Impute missing values using the most common value (mode)\n",
    "    # Replace missing values denoted as '?' with np.NaN, then fill it with mode\n",
    "    train.replace('?', np.NaN, inplace=True)\n",
    "    mode = train.mode().iloc[0]\n",
    "    train = train.fillna(mode)\n",
    "    \n",
    "    ########## PRINT DEBUG ##########\n",
    "    print(f\"TRAIN DATASET has {len(train.columns)} attributes, {len(train)} instances and {len(train['class'].unique())} unique classes.\")\n",
    "    print(f\"TEST and CROSS-VALIDATE DATASET has {len(test)} instances each.\")\n",
    "    # display(mode)\n",
    "    if len(cols_to_drop) > 0:\n",
    "          print(f\"No gain (all values in the attribute are the same) with: {','.join(cols_to_drop)}\")\n",
    "    ##########\n",
    "    \n",
    "    df = {\"train\": train, \"test\": test, \"crossval\": crossval}\n",
    "    \n",
    "          \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shit_preprocess(filename):\n",
    "    # Read in csv and add the column header\n",
    "    df = pd.read_csv(filename, header = None, names = set_column(filename))\n",
    "    nunique = df.apply(pd.Series.nunique)\n",
    "    cols_to_drop = nunique[nunique == 1].index\n",
    "    df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should build a supervised NB model\n",
    "def train(train_set):\n",
    "    \n",
    "    # Initialise dictionaries to hold the Prior and Posterior probabilities\n",
    "    priors = {}\n",
    "    posteriors = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))\n",
    "    \"\"\"Accessable using posteriors[class][attribute][value]\"\"\"\n",
    "    \n",
    "    # For each unique class label\n",
    "    for label in train_set['class'].unique():    \n",
    "        # Total number of instances\n",
    "        N = len(train_set)\n",
    "        \n",
    "        # Prior = No class label / N\n",
    "        prob = len(train_set.loc[train_set['class'] == label]) / N\n",
    "        priors[label] = prob\n",
    "        \n",
    "        # For each attribute, given a fixed class\n",
    "        for attribute in train_set.columns[:-1]:\n",
    "            temp = train_set.loc[train_set['class'] == label, [attribute,'class']]\n",
    "            \n",
    "            # Number of instances corresponding to the attribute, given a fixed class\n",
    "            n = len(temp)\n",
    "            \n",
    "            # Count all values (zzz iterative approach)\n",
    "            count = Counter(temp[attribute])\n",
    "            \n",
    "            # Insert into posterior dictionary\n",
    "            for i in count:\n",
    "                posteriors[label][attribute][i] = count[i] / n\n",
    "    \n",
    "    ############ PRINT DEBUG\n",
    "    # print(f\"Verifying that Axiom 2 has not been broken: {sum(priors.values())}\")\n",
    "    # printer(posteriors, prob)\n",
    "    ############\n",
    "    \n",
    "    model = {\"priors\": priors, \"posteriors\": posteriors}\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(filename):\n",
    "    df = shit_preprocess(filename)\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # 60 - 20 - 20 train, test, cross-val\n",
    "    partitions = []\n",
    "    k_inc = k = 4\n",
    "    \n",
    "    for i in range(k):\n",
    "        partitions.append(df.sample(frac=(1.0/k_inc)))\n",
    "        k_inc -= 1\n",
    "        df.drop(partitions[-1].index, axis=0, inplace=True)\n",
    "    \n",
    "    del df\n",
    "    return_dict = defaultdict(list)\n",
    "    # Run train/test for EACH partition, using partition as the test data and the rest as the training\n",
    "    for i in range(k):\n",
    "        # Copyright _blank_ @ https://gitgud._blank_.com/_blank_\n",
    "        test = partitions[i]\n",
    "        train = df_copy.iloc[df_copy.index.drop(test.index.values)]\n",
    "        return_dict[\"train\"].append(train)\n",
    "        return_dict[\"test\"].append(test)\n",
    "\n",
    "    return return_dict\n",
    "\n",
    "############### DEBUG ##################\n",
    "cross_pairs = cross_validation(datasets[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_train(cross_dict):\n",
    "    models = []\n",
    "    for i in range(len(cross_dict[\"train\"])):\n",
    "        models.append(train(cross_dict[\"train\"][i]))\n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_predict(cross_models, cross_pairs):\n",
    "    cross_tests = cross_pairs[\"test\"]\n",
    "    # display(cross_tests)\n",
    "    result = []\n",
    "    for i in range(len(cross_tests)):\n",
    "        # print(f\"{len(cross_models)}, {len(cross_tests)}\")\n",
    "        result.append(predict(cross_models[i], cross_tests[i]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_evaluate(cross_predicted, cross_pairs):\n",
    "    cross_tests = cross_pairs[\"test\"]\n",
    "    result = []\n",
    "    for i in range(len(cross_tests)):\n",
    "        result.append(evaluate(cross_predicted[i], cross_tests[i]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should predict the class for an instance or a set of instances, based on a trained model \n",
    "def predict(model, test_set):\n",
    "    priors = model[\"priors\"]\n",
    "    posteriors = model[\"posteriors\"]\n",
    "    \n",
    "    # Split into attributes, labels\n",
    "    test_labels = test_set['class']\n",
    "    test = test_set.drop('class', axis=1)\n",
    "    cols = test_set.columns\n",
    "    \n",
    "    # Probabilistic Smoothing\n",
    "    n = len(test_labels)\n",
    "    epsilon = 1e-100\n",
    "    \n",
    "    # Results\n",
    "    prediction = {}\n",
    "    \n",
    "    # Predicted labels to be returned\n",
    "    # Key = test instance index, Value = predicted label\n",
    "    predicted_labels = {}\n",
    "    \n",
    "    for i in range(n):\n",
    "        instance = test.iloc[i]\n",
    "        for label in priors.keys():       \n",
    "            prob = log(priors[label])\n",
    "            for attribute in cols:\n",
    "                try:\n",
    "                    # If value is non missing\n",
    "                    if instance[attribute] != '?':\n",
    "                        prob += log(posteriors[label][attribute][instance[attribute]])\n",
    "                    # Else ignore the value\n",
    "                    else:\n",
    "                        pass\n",
    "                except:\n",
    "                    # Value is 0 in the model\n",
    "                    prob += log(epsilon)\n",
    "            prediction[label] = prob\n",
    "        \n",
    "        ########## PRINT DEBUG\n",
    "        # print(f\"Test Instance: {i}, Predict: {max(prediction, key=prediction.get)}, Actual: {test_labels.iloc[i]}, RESULT: {max(prediction, key=prediction.get) == test_labels.iloc[i]}\")\n",
    "        ##########\n",
    "        \n",
    "        predicted_labels[i] = max(prediction, key=prediction.get)\n",
    "        \n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should evaluate a set of predictions, in a supervised context \n",
    "def evaluate(predicted_labels, test_set):\n",
    "    results = list()\n",
    "    \n",
    "    test_labels = test_set['class']\n",
    "    n = len(test_labels)\n",
    "    \n",
    "    for i in range(n):\n",
    "        if predicted_labels[i] == test_labels.iloc[i]:\n",
    "            results.append(1)\n",
    "        else:\n",
    "            results.append(0)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should calculate the Information Gain of an attribute or a set of attribute, with respect to the class\n",
    "def info_gain(train_set):\n",
    "    cols = train_set.columns[:-1]\n",
    "    label = train_set.columns[-1]\n",
    "    entropy = {}\n",
    "    \n",
    "    # for attribute in cols:\n",
    "        # entropy[attribute] = ENTROPY(ATTRIBUTE, LABEL)\n",
    "    \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing anneal.csv ...\n",
      "TRAIN DATASET has 29 attributes, 629 instances and 5 unique classes.\n",
      "TEST and CROSS-VALIDATE DATASET has 134 instances each.\n",
      "No gain (all values in the attribute are the same) with: product-type,m,marvi,corr,jurofm,s,p\n",
      "Accuracy for test set: 99.25%\n",
      "Accuracy for crossval set: 98.52%\n",
      "\n",
      "Overall accuracy: 98.88%\n",
      "Testing Accuracy on the Training Data: 99.11%\n",
      "2\n",
      "Testing accuracy with cross-validation: 98.66% 98.67% 99.55% 99.11%\n",
      "****************************************\n",
      "Processing breast-cancer.csv ...\n",
      "TRAIN DATASET has 10 attributes, 200 instances and 2 unique classes.\n",
      "TEST and CROSS-VALIDATE DATASET has 43 instances each.\n",
      "Accuracy for test set: 79.07%\n",
      "Accuracy for crossval set: 60.47%\n",
      "\n",
      "Overall accuracy: 69.77%\n",
      "Testing Accuracy on the Training Data: 75.52%\n",
      "2\n",
      "Testing accuracy with cross-validation: 77.78% 74.65% 68.06% 64.79%\n",
      "****************************************\n",
      "Processing car.csv ...\n",
      "TRAIN DATASET has 7 attributes, 1210 instances and 4 unique classes.\n",
      "TEST and CROSS-VALIDATE DATASET has 259 instances each.\n",
      "Accuracy for test set: 84.94%\n",
      "Accuracy for crossval set: 86.87%\n",
      "\n",
      "Overall accuracy: 85.91%\n",
      "Testing Accuracy on the Training Data: 87.38%\n",
      "2\n",
      "Testing accuracy with cross-validation: 87.04% 87.04% 82.41% 86.11%\n",
      "****************************************\n",
      "Processing cmc.csv ...\n",
      "TRAIN DATASET has 9 attributes, 1031 instances and 3 unique classes.\n",
      "TEST and CROSS-VALIDATE DATASET has 221 instances each.\n",
      "Accuracy for test set: 44.34%\n",
      "Accuracy for crossval set: 54.30%\n",
      "\n",
      "Overall accuracy: 49.32%\n",
      "Testing Accuracy on the Training Data: 50.58%\n",
      "2\n",
      "Testing accuracy with cross-validation: 45.11% 50.27% 51.09% 53.66%\n",
      "****************************************\n",
      "Processing hepatitis.csv ...\n",
      "TRAIN DATASET has 14 attributes, 109 instances and 2 unique classes.\n",
      "TEST and CROSS-VALIDATE DATASET has 23 instances each.\n",
      "Accuracy for test set: 86.96%\n",
      "Accuracy for crossval set: 82.61%\n",
      "\n",
      "Overall accuracy: 84.78%\n",
      "Testing Accuracy on the Training Data: 85.16%\n",
      "2\n",
      "Testing accuracy with cross-validation: 82.05% 79.49% 81.58% 87.18%\n",
      "****************************************\n",
      "Processing hypothyroid.csv ...\n",
      "TRAIN DATASET has 19 attributes, 2214 instances and 2 unique classes.\n",
      "TEST and CROSS-VALIDATE DATASET has 474 instances each.\n",
      "Accuracy for test set: 95.15%\n",
      "Accuracy for crossval set: 95.79%\n",
      "\n",
      "Overall accuracy: 95.47%\n",
      "Testing Accuracy on the Training Data: 95.23%\n",
      "2\n",
      "Testing accuracy with cross-validation: 97.09% 93.93% 94.56% 95.32%\n",
      "****************************************\n",
      "Processing mushroom.csv ...\n",
      "TRAIN DATASET has 22 attributes, 5687 instances and 2 unique classes.\n",
      "TEST and CROSS-VALIDATE DATASET has 1218 instances each.\n",
      "No gain (all values in the attribute are the same) with: veil-type\n",
      "Accuracy for test set: 99.51%\n",
      "Accuracy for crossval set: 99.18%\n",
      "\n",
      "Overall accuracy: 99.34%\n",
      "Testing Accuracy on the Training Data: 99.72%\n",
      "2\n",
      "Testing accuracy with cross-validation: 99.75% 99.70% 99.26% 99.95%\n",
      "****************************************\n",
      "Processing nursery.csv ...\n",
      "TRAIN DATASET has 9 attributes, 9072 instances and 5 unique classes.\n",
      "TEST and CROSS-VALIDATE DATASET has 1944 instances each.\n",
      "Accuracy for test set: 91.26%\n",
      "Accuracy for crossval set: 89.97%\n",
      "\n",
      "Overall accuracy: 90.61%\n",
      "Testing Accuracy on the Training Data: 90.31%\n",
      "2\n",
      "Testing accuracy with cross-validation: 90.34% 89.97% 90.68% 89.78%\n",
      "****************************************\n",
      "Processing primary-tumor.csv ...\n",
      "TRAIN DATASET has 18 attributes, 237 instances and 21 unique classes.\n",
      "TEST and CROSS-VALIDATE DATASET has 51 instances each.\n",
      "Accuracy for test set: 41.18%\n",
      "Accuracy for crossval set: 33.33%\n",
      "\n",
      "Overall accuracy: 37.25%\n",
      "Testing Accuracy on the Training Data: 60.47%\n",
      "2\n",
      "Testing accuracy with cross-validation: 43.53% 40.00% 51.19% 47.06%\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "# CROSS VALIDATION VERSION\n",
    "for data in datasets:\n",
    "    print(f\"Processing {data} ...\")\n",
    "    \n",
    "    df = preprocess(data)\n",
    "    same_df = shit_preprocess(data)\n",
    "    \n",
    "    # entropy = info_gain(df)\n",
    "    \n",
    "    model = train(df[\"train\"])\n",
    "    shit_model = train(same_df)\n",
    "    \n",
    "    # classification holds [(predicted label, actual label), results] for the test and crossval dataset\n",
    "    classificiation = defaultdict(list)\n",
    "    total_accuracy, total_n = 0, 0\n",
    "    \n",
    "    for i in [\"test\",\"crossval\"]:\n",
    "        predicted_labels = predict(model, df[i])\n",
    "        results = evaluate(predicted_labels, df[i])\n",
    "        \n",
    "        n = len(results)\n",
    "        correct = sum(results)\n",
    "        \n",
    "        total_accuracy += correct\n",
    "        total_n += n\n",
    "        \n",
    "        classificiation[i].append(predicted_labels)\n",
    "        classificiation[i].append(results)\n",
    "        \n",
    "        print(f\"Accuracy for {i} set: %.2f%%\" % (100*correct/n))\n",
    "    \n",
    "    A = predict(shit_model, same_df)\n",
    "    B = evaluate(A, same_df)\n",
    "    \n",
    "    print(f\"\\nOverall accuracy: %.2f%%\" % (100*total_accuracy/total_n))\n",
    "    print(\"Testing Accuracy on the Training Data: %.2f%%\" % (100*sum(B)/len(B)))\n",
    "    # Cross validation output\n",
    "    cross_pairs = cross_validation(data)    \n",
    "    cross_models = cross_train(cross_pairs)\n",
    "    cross_predicted = cross_predict(cross_models, cross_pairs)\n",
    "    cross_evaluated = cross_evaluate(cross_predicted, cross_pairs)\n",
    "    print(\"Testing accuracy with cross-validation: \" + \" \".join([\"%.2f%%\" % (100*sum(array) / len(array)) for array in cross_evaluated]))\n",
    "        \n",
    "    print(\"*\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions (you may respond in a cell or cells below):\n",
    "\n",
    "1. The Naive Bayes classifiers can be seen to vary, in terms of their effectiveness on the given datasets (e.g. in terms of Accuracy). Consider the Information Gain of each attribute, relative to the class distribution — does this help to explain the classifiers’ behaviour? Identify any results that are particularly surprising, and explain why they occur.\n",
    "2. The Information Gain can be seen as a kind of correlation coefficient between a pair of attributes: when the gain is low, the attribute values are uncorrelated; when the gain is high, the attribute values are correlated. In supervised ML, we typically calculate the Infomation Gain between a single attribute and the class, but it can be calculated for any pair of attributes. Using the pair-wise IG as a proxy for attribute interdependence, in which cases are our NB assumptions violated? Describe any evidence (or indeed, lack of evidence) that this is has some effect on the effectiveness of the NB classifier.\n",
    "3. Since we have gone to all of the effort of calculating Infomation Gain, we might as well use that as a criterion for building a “Decision Stump” (1-R classifier). How does the effectiveness of this classifier compare to Naive Bayes? Identify one or more cases where the effectiveness is notably different, and explain why.\n",
    "4. Evaluating the model on the same data that we use to train the model is considered to be a major mistake in Machine Learning. Implement a hold–out or cross–validation evaluation strategy. How does your estimate of effectiveness change, compared to testing on the training data? Explain why. (The result might surprise you!)\n",
    "5. Implement one of the advanced smoothing regimes (add-k, Good-Turing). Does changing the smoothing regime (or indeed, not smoothing at all) affect the effectiveness of the Naive Bayes classifier? Explain why, or why not.\n",
    "6. Naive Bayes is said to elegantly handle missing attribute values. For the datasets with missing values, is there any evidence that the performance is different on the instances with missing values, compared to the instances where all of the values are present? Does it matter which, or how many values are missing? Would a imputation strategy have any effect on this?\n",
    "\n",
    "Don't forget that groups of 1 student should respond to question (1), and one other question of your choosing. Groups of 2 students should respond to question (1) and question (2), and two other questions of your choosing. Your responses should be about 150-250 words each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Should improve since a using the training dataset to test a supervised algo is like a cheat. the supervised algo has already seen the answers and will recognise it. Using a test / cross-validation will ensure that we are left with unseen data albeit a decrease in training instances\n",
    "\n",
    "6. performance wise there should be no difference. there is no need to impute as a missing value could be significant in implying something. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
