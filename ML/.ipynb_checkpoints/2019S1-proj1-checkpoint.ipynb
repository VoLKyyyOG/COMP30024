{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2019 Semester 1\n",
    "-----\n",
    "## Project 1: Gaining Information about Naive Bayes\n",
    "-----\n",
    "###### Student Name(s): Akira and Callum\n",
    "###### Python version: 3.7.1 from Anaconda \n",
    "###### Submission deadline: 1pm, Fri 5 Apr 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you may use for your Project 1 submission. (You are not required to use it; in particular, there is no need to use iPython if you do not like it.)\n",
    "\n",
    "Marking will be applied on the five functions that are defined in this notebook, and to your responses to the questions at the end of this notebook.\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The cell below supresses forced output scrolling so you can see view the script output easier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We have specifically imported all functions below to ensure we implemented iteratively AND without the use of external functions wherever possible**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv, DataFrame, Series\n",
    "from collections import defaultdict, Counter\n",
    "from numpy import NaN\n",
    "from math import log\n",
    "\n",
    "########## POSSIBLE CSVs ##########\n",
    "d1 =  'anneal.csv'\n",
    "h1 = 'family,product-type,steel,carbon,hardness,temper_rolling,condition,formability,strength,non-ageing,surface-finish,surface-quality,enamelability,bc,bf,bt,bw-me,bl,m,chrom,phos,cbond,marvi,exptl,ferro,corr,bbvc,lustre,jurofm,s,p,shape,oil,bore,packing,class'.split(',')\n",
    "\n",
    "d2 =  'breast-cancer.csv'\n",
    "h2 = 'age,menopause,tumor-size,inv-nodes,node-caps,deg-malig,breast,breast-quad,irradiat,class'.split(',')\n",
    "\n",
    "d3 =  'car.csv'\n",
    "h3 = 'buying,maint,doors,persons,lug_boot,safety,class'.split(',')\n",
    "\n",
    "d4 =  'cmc.csv'\n",
    "h4 = 'w-education,h-education,n-child,w-relation,w-work,h-occupation,standard-of-living,media-exposure,class'.split(',')\n",
    "\n",
    "d5 =  'hepatitis.csv'\n",
    "h5 = 'sex,steroid,antivirals,fatigue,malaise,anorexia,liver-big,liver-firm,spleen-palpable,spiders,ascites,varices,histology,class'.split(',')\n",
    "\n",
    "d6 =  'hypothyroid.csv'\n",
    "h6 = 'sex,on-thyroxine,query-on-thyroxine,on_antithyroid,surgery,query-hypothyroid,query-hyperthyroid,pregnant,sick,tumor,lithium,goitre,TSH,T3,TT4,T4U,FTI,TBG,class'.split(',')\n",
    "\n",
    "d7 =  'mushroom.csv'\n",
    "h7 = 'cap-shape,cap-surface,cap-color,bruises,odor,gill-attachment,gill-spacing,gill-size,gill-color,stalk-shape,stalk-root,stalk-surface-above-ring,stalk-surface-below-ring,stalk-color-above-ring,stalk-color-below-ring,veil-type,veil-color,ring-number,ring-type,spore-print-color,population,habitat,class'.split(',')\n",
    "\n",
    "d8 =  'nursery.csv'\n",
    "h8 = 'parents,has_nurs,form,children,housing,finance,social,health,class'.split(',')\n",
    "\n",
    "d9 = 'primary-tumor.csv'\n",
    "h9 = 'age,sex,histologic-type,degree-of-diffe,bone,bone-marrow,lung,pleura,peritoneum,liver,brain,skin,neck,supraclavicular,axillar,mediastinum,abdominal,class'.split(',')\n",
    "\n",
    "datasets = [d1,d2,d3,d4,d5,d6,d7,d8,d9]\n",
    "dataset_headers = [h1,h2,h3,h4,h5,h6,h7,h8,h9]\n",
    "\n",
    "dictionary = {datasets[i] : dataset_headers[i] for i in range(len(datasets))}\n",
    "\n",
    "\"\"\"Sets the column of each dataset\"\"\"\n",
    "def set_column(filename):\n",
    "    return dictionary[filename]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Has been adjusted so that it works with testing on the train data, and partitioning for cross_val\"\"\"\n",
    "def preprocess(filename, testing_on_train = True, k = 10, drop = 'no', impute = 'no'):\n",
    "    # Add column headers, drop columns with only one unique value (all same value)\n",
    "    df = read_csv(filename, header = None, names = set_column(filename))\n",
    "    \n",
    "    if impute in 'yesYesYES':\n",
    "        df.replace('?', NaN, inplace=True)\n",
    "        mode = df.mode().iloc[0]\n",
    "        df = df.fillna(mode)\n",
    "    \n",
    "    \"\"\"If we are not using the cross validation method\"\"\"\n",
    "    # Return the whole dataset as a dataframe\n",
    "    if testing_on_train:\n",
    "        return df\n",
    "    else:\n",
    "        \"\"\"Drop no gain attributes given the input in the script\"\"\"\n",
    "        if drop in 'yesYesYES':\n",
    "            non_unique = df.apply(Series.nunique)\n",
    "            df.drop(non_unique[non_unique == 1].index, axis=1, inplace=True)\n",
    "            \n",
    "        temp = df.copy()\n",
    "        partitions = list()\n",
    "        \n",
    "        # k-fold Cross Validation\n",
    "        divisor = k\n",
    "        \n",
    "        for i in range(k):\n",
    "            partitions.append(temp.sample(frac=1/divisor))\n",
    "            divisor -= 1\n",
    "            temp.drop(partitions[-1].index, axis=0, inplace=True)\n",
    "        \n",
    "        del temp\n",
    "        \n",
    "        # Dictionary of train/test pairs\n",
    "        cross_validation_pairs = defaultdict(list)\n",
    "        models = list()\n",
    "        \n",
    "        for i in range(k):\n",
    "            test = partitions[i]\n",
    "            train = df.iloc[df.index.drop(test.index.values)]\n",
    "            cross_validation_pairs[\"train\"].append(train)\n",
    "            cross_validation_pairs[\"test\"].append(test)\n",
    "            \n",
    "        return cross_validation_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Trains a model given a training dataset\"\"\"\n",
    "def train(train_set):\n",
    "    N = len(train_set)\n",
    "    priors = {}\n",
    "    posteriors = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))\n",
    "    \"\"\"Accessable using posteriors[class j][attribute x][value i]\"\"\"\n",
    "    \n",
    "    for label in train_set['class'].unique():\n",
    "        priors[label] = len(train_set.loc[train_set['class'] == label]) / N\n",
    "        for attribute in train_set.columns[:-1]:\n",
    "            temp = train_set.loc[train_set['class'] == label, [attribute,'class']]\n",
    "            n = len(temp)\n",
    "            count = Counter(temp[attribute])\n",
    "            for i in count:\n",
    "                posteriors[label][attribute][i] = count[i] / n\n",
    "                \n",
    "    trained_model = {\"priors\": priors, \"posteriors\": posteriors}\n",
    "    \n",
    "    return trained_model\n",
    "\n",
    "\"\"\"Trains M partitions for cross_val\"\"\"\n",
    "def cross_validation_train(cross_validation_pairs):\n",
    "    trained_models = list()\n",
    "    train_set = cross_validation_pairs[\"train\"]\n",
    "    test_set = cross_validation_pairs[\"test\"]\n",
    "    N = len(train_set)\n",
    "    \n",
    "    for i in range(N):\n",
    "        trained_models.append(train(train_set[i]))\n",
    "        \n",
    "    return trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Predicts a test set\"\"\"\n",
    "def predict(trained_model, test_set):\n",
    "    priors = trained_model[\"priors\"]\n",
    "    posteriors = trained_model[\"posteriors\"]\n",
    "    \n",
    "    \"\"\"Drop the class labels of the test set\"\"\"\n",
    "    test_labels = test_set['class']\n",
    "    test = test_set.drop('class', axis=1)\n",
    "    cols = test_set.columns\n",
    "    \n",
    "    \"\"\"Probabilistic Smoothing with epsilon -> 0\"\"\"\n",
    "    n = len(test_labels)\n",
    "    epsilon = 1e-100\n",
    "    \n",
    "    \"\"\"Model Prediction\"\"\"\n",
    "    prediction = {}\n",
    "    \n",
    "    \"\"\"The Predicted Labels to be Returned\"\"\"\n",
    "    \"\"\"(Key, Value) = (Test Instance Row, Predicted Label)\"\"\"\n",
    "    predicted_labels = {}\n",
    "    \n",
    "    for i in range(n):\n",
    "        instance = test.iloc[i]\n",
    "        for label in priors.keys():       \n",
    "            prob = log(priors[label])/log(2)\n",
    "            for attribute in cols:\n",
    "                try:\n",
    "                    \"\"\"If the value is non missing\"\"\"\n",
    "                    if instance[attribute] != '?':\n",
    "                        prob += log(posteriors[label][attribute][instance[attribute]])/log(2)\n",
    "                    else:\n",
    "                        \"\"\"Otherwise we have chosen to simply ignore it\"\"\"\n",
    "                        pass\n",
    "                except:\n",
    "                    \"\"\"If the value does not exist in our model, we use epsilon\"\"\"\n",
    "                    prob += log(epsilon)/log(2)\n",
    "                    \n",
    "            prediction[label] = prob\n",
    "        \n",
    "        \"\"\"Choose the predicted class with the highest probability\"\"\"\n",
    "        predicted_labels[i] = max(prediction, key=prediction.get)\n",
    "        \n",
    "    return predicted_labels\n",
    "\n",
    "\"\"\"Tests partitions\"\"\"\n",
    "def cross_validation_predict(trained_models, cross_validation_pairs):\n",
    "    test_set = cross_validation_pairs[\"test\"]\n",
    "    N = len(test_set)\n",
    "    predictions = list()\n",
    "    \n",
    "    for i in range(N):\n",
    "        predictions.append(predict(trained_models[i], test_set[i]))\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Evaluates the accuracy\"\"\"\n",
    "def evaluate(predicted_labels, test_set):\n",
    "    test_labels = test_set['class']\n",
    "    n = len(test_labels)\n",
    "    \n",
    "    return [1 if predicted_labels[i] == test_labels.iloc[i] else 0 for i in range(n)]\n",
    "\n",
    "\"\"\"Evaluates cross_val accuracy\"\"\"\n",
    "def cross_validation_evaluate(predictions, cross_pairs):\n",
    "    test = cross_pairs[\"test\"]\n",
    "    N = len(test)\n",
    "    results = list()\n",
    "    for i in range(N):\n",
    "        results.append(evaluate(predictions[i], test[i]))\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Calculates the entropy given a series\"\"\"\n",
    "def entropy(distribution):\n",
    "    # Calculates the entropy (uncertainty) of an event (class) for a given distribution \n",
    "    # (e.g. distribution given a certain attribute value)\n",
    "    N = len(distribution)\n",
    "    # Normalise and count values iteratively\n",
    "    event = [i/N for i in Counter(distribution).values()]\n",
    "    \n",
    "    # Will lose marks for this since it is an aggregate function according to forums\n",
    "    # The iterative approach is off by 0.000000000000000xx compared to the .value_counts() method\n",
    "    # event = pd.Series(attribute_value).value_counts(normalize=True, sort=False)\n",
    "    \n",
    "    return (-1*sum([i*log(i)/log(2) for i in event]))\n",
    "\n",
    "\"\"\"Calculates the mean information given a dataset\"\"\"\n",
    "def mean_info(dataset):\n",
    "    mean_info_per_attribute = defaultdict(float)\n",
    "    \n",
    "    for attribute in dataset.columns[:-1]:\n",
    "        # Used to calculate 'probability' of said value for a random instance (number of values / total number of instances)\n",
    "        attribute_value_counts = Counter(dataset[attribute])\n",
    "        N = len(dataset[attribute])\n",
    "        for value in dataset[attribute].unique():\n",
    "            # dataframe.loc on said value and return the corresponding class column\n",
    "            corresponding_values = dataset.loc[dataset[attribute] == value, 'class']\n",
    "            # add (probability of said value * entropy of said value) to the attribute's mean info\n",
    "            mean_info_per_attribute[attribute] += attribute_value_counts[value]/N * entropy(corresponding_values)\n",
    "            \n",
    "    return mean_info_per_attribute\n",
    "\n",
    "\"\"\"Calculates the information gain given a dataset. Adjusted so that it can drop 0 info_gain columns\"\"\"\n",
    "def info_gain(dataset, drop_no_gain = False):\n",
    "    mean_info_per_attribute = mean_info(dataset) # Weighted sum of child stump entropies if split on each attribute\n",
    "    class_entropy = entropy(dataset['class']) # H(R), the entropy of class distribution prior to splitting\n",
    "    info_gain_given_class = defaultdict(float)\n",
    "    \n",
    "    for attribute in mean_info_per_attribute:\n",
    "        # Calculates IG(attribute) for the dataset's class\n",
    "        info_gain_given_class[attribute] = class_entropy - mean_info_per_attribute[attribute]\n",
    "    \n",
    "    \"\"\"If we want to drop the columns with absolutely 0 information gain\"\"\"\n",
    "    if drop_no_gain:\n",
    "        non_unique = dataset.apply(Series.nunique)\n",
    "        dataset.drop(non_unique[non_unique == 1].index, axis=1, inplace=True)\n",
    "        return dataset\n",
    "    else:\n",
    "        return info_gain_given_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here is a script that runs every dataset and tests on its training, as well as k-fold cross-validation for a given _k_. It will also ask (y/n) for printing relevant information gain, and to drop attributes with 0 information gain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "Enter k value for k-Fold Cross Validation: 10\n",
      "Drop all columns with absolutely no information gain? (y/n): y\n",
      "Print the information gain? (y/n): n\n",
      "Impute missing values? (y/n): y\n",
      "****************************************\n",
      "Processing anneal.csv ...\n",
      "TESTING ON THE TRAIN DATA\n",
      "Accuracy for Testing on the Training Data: 99.11%\n",
      "\n",
      "10-FOLD CROSS VALIDATION\n",
      "...\n",
      "Accuracy using k-Fold Cross Validation: 100.00% 100.00% 98.89% 100.00% 96.67% 97.78% 100.00% 100.00% 96.67% 100.00%\n",
      "Average 10-Fold Cross Validation Accuracy: 99.00%\n",
      "****************************************\n",
      "Processing breast-cancer.csv ...\n",
      "TESTING ON THE TRAIN DATA\n",
      "Accuracy for Testing on the Training Data: 75.87%\n",
      "\n",
      "10-FOLD CROSS VALIDATION\n",
      "...\n",
      "Accuracy using k-Fold Cross Validation: 72.41% 62.07% 75.00% 79.31% 75.00% 68.97% 75.00% 58.62% 89.29% 65.52%\n",
      "Average 10-Fold Cross Validation Accuracy: 72.12%\n",
      "****************************************\n",
      "Processing car.csv ...\n",
      "TESTING ON THE TRAIN DATA\n",
      "Accuracy for Testing on the Training Data: 87.38%\n",
      "\n",
      "10-FOLD CROSS VALIDATION\n",
      "...\n",
      "Accuracy using k-Fold Cross Validation: 84.39% 86.71% 85.55% 83.82% 88.44% 84.97% 83.72% 84.97% 87.21% 86.13%\n",
      "Average 10-Fold Cross Validation Accuracy: 85.59%\n",
      "****************************************\n",
      "Processing cmc.csv ...\n",
      "TESTING ON THE TRAIN DATA\n",
      "Accuracy for Testing on the Training Data: 50.58%\n",
      "\n",
      "10-FOLD CROSS VALIDATION\n",
      "...\n",
      "Accuracy using k-Fold Cross Validation: 44.90% 46.94% 43.54% 55.10% 43.24% 53.06% 53.38% 49.66% 49.32% 55.78%\n",
      "Average 10-Fold Cross Validation Accuracy: 49.49%\n",
      "****************************************\n",
      "Processing hepatitis.csv ...\n",
      "TESTING ON THE TRAIN DATA\n",
      "Accuracy for Testing on the Training Data: 84.52%\n",
      "\n",
      "10-FOLD CROSS VALIDATION\n",
      "...\n",
      "Accuracy using k-Fold Cross Validation: 81.25% 86.67% 81.25% 93.33% 87.50% 80.00% 87.50% 86.67% 75.00% 93.33%\n",
      "Average 10-Fold Cross Validation Accuracy: 85.25%\n",
      "****************************************\n",
      "Processing hypothyroid.csv ...\n",
      "TESTING ON THE TRAIN DATA\n",
      "Accuracy for Testing on the Training Data: 95.23%\n",
      "\n",
      "10-FOLD CROSS VALIDATION\n",
      "...\n",
      "Accuracy using k-Fold Cross Validation: 94.94% 95.25% 95.25% 94.30% 93.99% 98.11% 94.62% 96.53% 94.62% 94.64%\n",
      "Average 10-Fold Cross Validation Accuracy: 95.22%\n",
      "****************************************\n",
      "Processing mushroom.csv ...\n",
      "TESTING ON THE TRAIN DATA\n",
      "Accuracy for Testing on the Training Data: 99.58%\n",
      "\n",
      "10-FOLD CROSS VALIDATION\n",
      "...\n",
      "Accuracy using k-Fold Cross Validation: 99.63% 99.63% 99.63% 99.38% 99.88% 99.51% 99.88% 100.00% 99.75% 99.63%\n",
      "Average 10-Fold Cross Validation Accuracy: 99.69%\n",
      "****************************************\n",
      "Processing nursery.csv ...\n",
      "TESTING ON THE TRAIN DATA\n",
      "Accuracy for Testing on the Training Data: 90.31%\n",
      "\n",
      "10-FOLD CROSS VALIDATION\n",
      "...\n",
      "Accuracy using k-Fold Cross Validation: 90.97% 89.97% 91.28% 89.20% 91.36% 90.51% 89.20% 88.66% 90.74% 91.74%\n",
      "Average 10-Fold Cross Validation Accuracy: 90.36%\n",
      "****************************************\n",
      "Processing primary-tumor.csv ...\n",
      "TESTING ON THE TRAIN DATA\n",
      "Accuracy for Testing on the Training Data: 57.52%\n",
      "\n",
      "10-FOLD CROSS VALIDATION\n",
      "...\n",
      "Accuracy using k-Fold Cross Validation: 38.24% 52.94% 50.00% 58.82% 47.06% 47.06% 47.06% 32.35% 44.12% 63.64%\n",
      "Average 10-Fold Cross Validation Accuracy: 48.13%\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "print(\"*\"*40)\n",
    "k = int(input(\"Enter k value for k-Fold Cross Validation: \"))\n",
    "\"\"\"DO WE WANT TO DROP 0 INFO GAIN ATTRIBUTES?\"\"\"\n",
    "drop = input(\"Drop all columns with absolutely no information gain? (y/n): \").lower()\n",
    "\"\"\"DO WE WANT TO PRINT THE INFO GAIN?\"\"\"\n",
    "to_print = input(\"Print the information gain? (y/n): \").lower()\n",
    "\"\"\"DO WE WANT TO IMPUTE MISSING VALUES FOR THE TRAINING SET\"\"\"\n",
    "to_impute = input(\"Impute missing values? (y/n): \").lower()\n",
    "print(\"*\"*40)\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "for data in datasets:\n",
    "    print(f\"Processing {data} ...\")\n",
    "    print(\"TESTING ON THE TRAIN DATA\")\n",
    "    \n",
    "    df = preprocess(data, impute = to_impute)\n",
    "    if drop in 'yesYesYES':\n",
    "        df = info_gain(df, drop_no_gain = True)\n",
    "        info_gain_given_class = info_gain(df)\n",
    "    else:\n",
    "        info_gain_given_class = info_gain(df)\n",
    "    if to_print in 'yesYesYES':\n",
    "        for attribute in info_gain_given_class:\n",
    "            print(f'InfoGain({attribute} | class) = {info_gain_given_class[attribute]:.4f}')\n",
    "        print('...')\n",
    "\n",
    "    \"\"\"TRAIN / TEST\"\"\"\n",
    "    model = train(df)\n",
    "    prediction = predict(model, df)\n",
    "    results = evaluate(prediction, df)\n",
    "    print(f\"Accuracy for Testing on the Training Data: {100*sum(results)/len(results):.2f}%\")\n",
    "\n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "    print(f\"\\n{k}-FOLD CROSS VALIDATION\")\n",
    "\n",
    "    cross_validation_pairs = preprocess(data, testing_on_train = False, k = k, drop = drop)\n",
    "    trained_models = cross_validation_train(cross_validation_pairs)\n",
    "    print(\"...\")\n",
    "    predictions = cross_validation_predict(trained_models, cross_validation_pairs)\n",
    "    cross_validation_results = cross_validation_evaluate(predictions, cross_validation_pairs)\n",
    "    print(\"Accuracy using k-Fold Cross Validation: \" + \" \".join([f\"{100*sum(i) / len(i):.2f}%\" for i in cross_validation_results]))\n",
    "    print(f\"Average {k}-Fold Cross Validation Accuracy: {sum([100*sum(i) / len(i) for i in cross_validation_results]) / len(cross_validation_results):.2f}%\")\n",
    "    print(\"*\"*40)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Questions 1, 2, 4 and 6 (150 - 200 words for each response):\n",
    "\n",
    "#### 1. The Naive Bayes classifiers can be seen to vary, in terms of their effectiveness on the given datasets (e.g. in terms of Accuracy). Consider the Information Gain of each attribute, relative to the class distribution — does this help to explain the classifiers’ behaviour? Identify any results that are particularly surprising, and explain why they occur.\n",
    "    - According to our Information Gain for each attribute, we find that if there is ONE attribute with SIGNIFICANTLY MORE information gain COMPARED to the other attributes, we result in a MUCH HIGHER ACCURACY\n",
    "    - If all the information gains have SIMILAR values, then the model does not seem to perform as well\n",
    "        - Can be seen with the `primary-tumor.csv` where all the information gain is roughly between 0.1 - 0.2\n",
    "        - Compare that to `mushroom.csv` where there is one attribute with 0.9 information gain, and the rest between 0.1 - 0.4 and some less than 0.1\n",
    "    - There are exceptions however - note 'hypothyroid.csv' whose attributes are very little information gain despite high Naive Bayes classification errors.y\n",
    "    \n",
    "\n",
    "#### 2. The Information Gain can be seen as a kind of correlation coefficient between a pair of attributes: when the gain is low, the attribute values are uncorrelated; when the gain is high, the attribute values are correlated. In supervised ML, we typically calculate the Infomation Gain between a single attribute and the class, but it can be calculated for any pair of attributes. Using the pair-wise IG as a proxy for attribute interdependence, in which cases are our NB assumptions violated? Describe any evidence (or indeed, lack of evidence) that this is has some effect on the effectiveness of the NB classifier.\n",
    "    - Using the results below (its pretty messy yikes)\n",
    "    - See that the information gain between a pair of attributes is consistent\n",
    "    - NB assumptions violated in a sense that we have assumed that each class is independent of eachother. Yet if we look at the information gain between pairs of attributes, we see that some are not just dependent, but significantly depndent on another attribute\n",
    "        - See InfoGain(another_attribute | attribute = steel) = 1.9 and compare it to InfoGain(another_attribute | attribute = product-type) = 0\n",
    " \n",
    "    \n",
    "\n",
    "#### 4. Evaluating the model on the same data that we use to train the model is considered to be a major mistake in Machine Learning. Implement a hold–out or cross–validation evaluation strategy. How does your estimate of effectiveness change, compared to testing on the training data? Explain why. (The result might surprise you!)\n",
    "    - Implemented a k-fold Cross-Validation strategy\n",
    "    - Estimate of effectiveness does not change significantly (typically < 2% difference in accuracy where k = 10 in table below)\n",
    "    - Although each instance will become a training instance and testing instance at one stage, there are better partitions to train on compared to others (limited to the fact that we are taking random samples of the dataset each time)\n",
    "    \n",
    "\n",
    "#### 6. Naive Bayes is said to elegantly handle missing attribute values. For the datasets with missing values, is there any evidence that the performance is different on the instances with missing values, compared to the instances where all of the values are present? Does it matter which, or how many values are missing? Would a imputation strategy have any effect on this?\n",
    "    - To an extent, it does matter how many values are missing, but it is also dependent if the attribute has a high information gain or not\n",
    "    - Missing values in itself could be siginficiant (i.e. Since there is a missing value, it could lead to a specific label class)\n",
    "    - See table below for results\n",
    "        - Can see that training has no effect whether we impute values or not\n",
    "        - However, we can see that the overall accuracy increases slightly when testing using the cross-validation method\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 6 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No Imputations and No dropping 0 info gain</th>\n",
       "      <th>10 fold Cross Validation result 1</th>\n",
       "      <th>No Imputations and Dropping 0 info gain</th>\n",
       "      <th>10 fold Cross Validation result 2</th>\n",
       "      <th>Mode Imputations and No dropping 0 info gain</th>\n",
       "      <th>10 fold Cross Validation result 3</th>\n",
       "      <th>Mode Imputations and Dropping 0 info gain</th>\n",
       "      <th>10 fold Cross Validation result 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99.11</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.11</td>\n",
       "      <td>98.99</td>\n",
       "      <td>99.11</td>\n",
       "      <td>98.78</td>\n",
       "      <td>99.11</td>\n",
       "      <td>99.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75.52</td>\n",
       "      <td>71.29</td>\n",
       "      <td>75.52</td>\n",
       "      <td>72.77</td>\n",
       "      <td>75.85</td>\n",
       "      <td>72.01</td>\n",
       "      <td>75.87</td>\n",
       "      <td>71.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87.38</td>\n",
       "      <td>85.42</td>\n",
       "      <td>87.38</td>\n",
       "      <td>85.71</td>\n",
       "      <td>87.38</td>\n",
       "      <td>85.82</td>\n",
       "      <td>87.38</td>\n",
       "      <td>85.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50.58</td>\n",
       "      <td>49.08</td>\n",
       "      <td>50.58</td>\n",
       "      <td>49.70</td>\n",
       "      <td>50.58</td>\n",
       "      <td>49.15</td>\n",
       "      <td>50.58</td>\n",
       "      <td>49.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85.16</td>\n",
       "      <td>83.71</td>\n",
       "      <td>85.16</td>\n",
       "      <td>83.17</td>\n",
       "      <td>84.52</td>\n",
       "      <td>84.50</td>\n",
       "      <td>84.52</td>\n",
       "      <td>84.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>95.23</td>\n",
       "      <td>95.23</td>\n",
       "      <td>95.23</td>\n",
       "      <td>95.23</td>\n",
       "      <td>95.23</td>\n",
       "      <td>95.23</td>\n",
       "      <td>95.23</td>\n",
       "      <td>95.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>99.72</td>\n",
       "      <td>99.69</td>\n",
       "      <td>99.72</td>\n",
       "      <td>99.68</td>\n",
       "      <td>99.58</td>\n",
       "      <td>99.68</td>\n",
       "      <td>99.58</td>\n",
       "      <td>99.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>90.31</td>\n",
       "      <td>90.32</td>\n",
       "      <td>90.31</td>\n",
       "      <td>90.25</td>\n",
       "      <td>90.31</td>\n",
       "      <td>90.33</td>\n",
       "      <td>90.31</td>\n",
       "      <td>90.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>60.47</td>\n",
       "      <td>46.02</td>\n",
       "      <td>60.47</td>\n",
       "      <td>46.31</td>\n",
       "      <td>57.52</td>\n",
       "      <td>46.90</td>\n",
       "      <td>57.52</td>\n",
       "      <td>48.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   No Imputations and No dropping 0 info gain  \\\n",
       "0                                       99.11   \n",
       "1                                       75.52   \n",
       "2                                       87.38   \n",
       "3                                       50.58   \n",
       "4                                       85.16   \n",
       "5                                       95.23   \n",
       "6                                       99.72   \n",
       "7                                       90.31   \n",
       "8                                       60.47   \n",
       "\n",
       "   10 fold Cross Validation result 1  No Imputations and Dropping 0 info gain  \\\n",
       "0                              99.00                                    99.11   \n",
       "1                              71.29                                    75.52   \n",
       "2                              85.42                                    87.38   \n",
       "3                              49.08                                    50.58   \n",
       "4                              83.71                                    85.16   \n",
       "5                              95.23                                    95.23   \n",
       "6                              99.69                                    99.72   \n",
       "7                              90.32                                    90.31   \n",
       "8                              46.02                                    60.47   \n",
       "\n",
       "   10 fold Cross Validation result 2  \\\n",
       "0                              98.99   \n",
       "1                              72.77   \n",
       "2                              85.71   \n",
       "3                              49.70   \n",
       "4                              83.17   \n",
       "5                              95.23   \n",
       "6                              99.68   \n",
       "7                              90.25   \n",
       "8                              46.31   \n",
       "\n",
       "   Mode Imputations and No dropping 0 info gain  \\\n",
       "0                                         99.11   \n",
       "1                                         75.85   \n",
       "2                                         87.38   \n",
       "3                                         50.58   \n",
       "4                                         84.52   \n",
       "5                                         95.23   \n",
       "6                                         99.58   \n",
       "7                                         90.31   \n",
       "8                                         57.52   \n",
       "\n",
       "   10 fold Cross Validation result 3  \\\n",
       "0                              98.78   \n",
       "1                              72.01   \n",
       "2                              85.82   \n",
       "3                              49.15   \n",
       "4                              84.50   \n",
       "5                              95.23   \n",
       "6                              99.68   \n",
       "7                              90.33   \n",
       "8                              46.90   \n",
       "\n",
       "   Mode Imputations and Dropping 0 info gain  \\\n",
       "0                                      99.11   \n",
       "1                                      75.87   \n",
       "2                                      87.38   \n",
       "3                                      50.58   \n",
       "4                                      84.52   \n",
       "5                                      95.23   \n",
       "6                                      99.58   \n",
       "7                                      90.31   \n",
       "8                                      57.52   \n",
       "\n",
       "   10 fold Cross Validation result 4  \n",
       "0                              99.00  \n",
       "1                              71.59  \n",
       "2                              85.07  \n",
       "3                              49.15  \n",
       "4                              84.00  \n",
       "5                              95.23  \n",
       "6                              99.68  \n",
       "7                              90.37  \n",
       "8                              48.40  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "display(read_csv('results.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 2 script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entropy(distribution):\n",
    "    N = len(distribution)\n",
    "    event = [i/N for i in Counter(distribution).values()]\n",
    "    return (-1*sum([i*log(i)/log(2) for i in event]))\n",
    "def mean_info(dataset, column_name):\n",
    "    mean_info_per_attribute = defaultdict(float)\n",
    "    for attribute in dataset.columns[:-1]:\n",
    "        attribute_value_counts = Counter(dataset[attribute])\n",
    "        N = len(dataset[attribute])\n",
    "        for value in dataset[attribute].unique():\n",
    "            corresponding_values = dataset.loc[dataset[attribute] == value, column_name]\n",
    "            mean_info_per_attribute[attribute] += attribute_value_counts[value]/N * entropy(corresponding_values)\n",
    "    return mean_info_per_attribute\n",
    "def info_gain_attributes(dataset, to_print = 'no'):\n",
    "    dataset.drop('class', axis=1, inplace=True)\n",
    "    entropy_per_attribute = defaultdict(float)\n",
    "    all_IG = defaultdict(float) # Me change\n",
    "    for attribute in dataset.columns:\n",
    "        entropy_per_attribute[attribute] = entropy(dataset[attribute])\n",
    "        mean_info_per_attribute = mean_info(dataset, attribute)\n",
    "    for attribute in entropy_per_attribute:\n",
    "        if to_print in 'yesYesYES': # Me change\n",
    "            print(f\"Entropy({attribute}) = {entropy_per_attribute[attribute]:.4f}\\n\")\n",
    "        average = list()\n",
    "        for every_other_attribute in mean_info_per_attribute:\n",
    "            info = entropy_per_attribute[attribute] - mean_info_per_attribute[every_other_attribute]\n",
    "            average.append(info)\n",
    "            if to_print in 'yesYesYES':\n",
    "                print(f\"InfoGain({every_other_attribute} | {attribute}) = {info:.4f}\")\n",
    "        \n",
    "        if to_print in 'yesYesYES': # Me change\n",
    "            print(f\"\\nAverage Info Gain for {attribute} to every other attribute is {sum(average)/len(mean_info_per_attribute):.4f}\")\n",
    "            print(\"*\"*40)\n",
    "        \n",
    "        all_IG[attribute]= (sum(average)/len(mean_info_per_attribute)) # Me change\n",
    "    return all_IG # Me change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For anneal.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.40809229878317, var 0.3170490525685071'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For breast-cancer.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.7785422508877268, var 0.46400744081345496'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For car.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.20751874963942205, var 0.0430640314519091'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For cmc.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.027816103414055, var 0.45311689187359844'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For hepatitis.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'-0.05531921244034507, var 0.043267474135784766'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For hypothyroid.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.04001974280781287, var 0.06363846847825899'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For mushroom.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'-0.5244453254650423, var 0.6579670385814322'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For nursery.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.12275976150034232, var 0.13865649918762546'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For primary-tumor.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'-0.08628197458177905, var 0.14868398296286614'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from numpy import var, array\n",
    "for data in datasets:\n",
    "    to_print = 'n' #input(\"Print InfoGain(every other attribute | attribute=A)? (y/n): \") # me change\n",
    "    df = preprocess(data)\n",
    "    IG_per_attr = info_gain_attributes(df, to_print = to_print) # Me change\n",
    "    print(f\"For {data}\") # Me change\n",
    "    display(f\"{sum(IG_per_attr.values())/len(IG_per_attr)}, var {var(array(list(IG_per_attr.values())))}\") # Me change\n",
    "    # break # me change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
